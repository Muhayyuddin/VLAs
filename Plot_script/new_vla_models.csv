
Model,End_to_End,Component_Focused,Main_Contribution,Dataset,Vision_Encoder,Language_Encoder,Action_Decoder,FusionDepth,FusionType,DecoderFamily,Domain,VisionParams,LLMParams,CTask,CMod,LogN,Success,Adjusted_Success_0to1,Success_Provenance,Difficulty_Index,Generalization_Index_0to1,Generalization_Index_Recomputed,Success_Rate_Adjusted,Generalization_Index_Old
CLIPort,Yes,Yes,Pioneered the semantic grounding of visuomotor policies by integrating CLIP features into dense transport maps for precise pick-and-place.,Self-collected visuomotor demos,CLIP-ResNet50 + Transporter-ResNet,CLIP text encoder,LingUNet,early,additive,autoregressive,manipulation,7.0,7.0,0.7127598674935811,0.5237479914465233,3.580369559845169,0.1173249066721428,0.3525304888455273,proxy_existing,0.0,0.12,0.3898637267015024,0.687538607082158,0.12
RT-1,Yes,Yes,Introduced a discretized action transformer for scalable multi-task kitchen manipulation.,Self-collected RT-1-Kitchen,EfficientNet CNN,Universal Sentence Encoder,Discretized action transformer head,late,additive,autoregressive,manipulation,7.0,7.5,0.7229760638914537,0.7925844786523562,4.914731676171138,0.0811252643996296,0.3151200916749392,proxy_existing,0.0,0.12,0.6315372281634263,0.6513389648096448,0.12
Gato,Yes,Yes,"Demonstrated a unified tokenization scheme across vision, language, and control tasks, achieving zero-shot transfer across domains.",Self-collected multi-domain tasks,custom ViT,Sentence Piece tokenizer,Autoregressive Transformer,mid,additive,autoregressive,manipulation,8.0,7.5,0.8974576185571919,0.8359025511407572,3.3239110568563475,-0.1100576750589249,0.1299884343793348,proxy_existing,0.3,0.12,0.3898637267015024,0.4601560253510903,0.12
VIMA,Yes,Yes,Handled six distinct vision-language grounding tasks via a prompt-based multimodal policy.,VIMA self-collected,Mask R-CNN,T5-base,Transformer policy head,late,additive,autoregressive,manipulation,7.0,8.0,0.7991721913128478,0.8380069757685371,6.399450830221941,0.0118810966338154,0.2435599420368611,proxy_existing,0.0,0.12,0.3898637267015024,0.5820947970438306,0.12
PerAct,Yes,No,Uses voxel-based representation with language conditioning for high-precision manipulation; operates directly on point cloud voxels.,RLBench,Perceiver Transformer + voxel grid encoder,CLIP text encoder,Transformer voxel policy head,late,additive,autoregressive,manipulation,7.5,7.0,0.4591508171188656,0.6574843244062099,6.160410617474682,-0.0523324853515879,0.177198634161614,proxy_existing,0.0,0.12,0.3015061411624024,0.5178812150584273,0.12
SayCan,No,Yes,Combines language model planning with value function grounding in the real world; interprets high-level goals into executable robot actions.,Self-collected everyday manipulation demos,none,PaLM,Value-conditioned execution module,late,additive,autoregressive,manipulation,7.5,7.5,0.734586877041389,0.4004486741562342,6.597642792397352,0.0186350725326229,0.281488851482133,proxy_existing,0.35,0.32,0.3898637267015024,0.588848772942638,0.32
RoboAgent,Yes,No,MT-ACT: multi-task transformer policy with semantically augmented CVAE encoding and action-chunking for strong real-world generalization.,RoboSet teleop demos,Multi-view CNN encoder,Semantic transformer encoder,CVAE + Chunked trajectory predictor,late,additive,autoregressive,manipulation,7.5,7.5,0.8449101813042241,0.7252329205938499,4.560081470746022,0.1842666858742209,0.4217112428541688,proxy_existing,0.0,0.12,0.5344293229142001,0.7544803862842361,0.12
RT-Trajectory,Yes,No,Conditioned policies on user-sketched trajectories to improve generalization to novel layouts and paths.,RT-1 dataset,EfficientNet-B3,,Sketch-conditioned behavioral cloning policy,late,additive,autoregressive,manipulation,7.0,7.5,0.6418766670542628,0.7605357478913903,3.8338340004908176,0.0241846148409261,0.2834099680141348,proxy_existing,0.3,0.12,0.6315372281634263,0.5943983152509413,0.12
ACT,Yes,Yes,Applied temporal ensembling to achieve smooth bimanual manipulation with 0.1 mm precision.,self-collected demos on ALOHA,ResNet-18,none,CVAE-Transformer head,late,additive,autoregressive,manipulation,7.0,7.5,0.5831591106725926,0.7786948181947049,6.689238755674514,-0.1450580825488966,0.0813716811514936,proxy_existing,0.0,0.12,0.3898637267015024,0.42515561786111855,0.12
RT-2,Yes,Yes,"First large VLA co-finetuned on Internet VQA and robot data, unlocking emergent multi-robot zero-shot capabilities.",Internet VQA + RT-1-Kitchen,PaLI-X/PaLM-E ViT,PaLI-X/PaLM-E text encoder,Symbol-tuning transformer,late,additive,autoregressive,manipulation,8.0,7.5,0.8284117485771623,0.7932610098464953,3.5127330152894785,0.0846149942550634,0.3693478153018754,proxy_existing,0.4499999999999999,0.64,0.6315372281634263,0.6548286946650785,0.64
VoxPoser,Yes,Yes,Achieved zero-shot constraint-aware motion planning by composing a frozen VLM and LLM without additional training.,Self-collected motion demos+RLBench,OWL-ViT,GPT-4,MPC optimizer,late,additive,autoregressive,manipulation,8.0,7.5,0.3400593849515136,0.713772575414488,4.99268770790803,0.2066548795114432,0.5469016664199399,proxy_existing,0.65,0.44,0.3015061411624024,0.7768685799214584,0.44
CLIP-RT,No,Yes,"Contrastive policy using CLIP vision and text encoders to select language-based motion primitives, enabling fast learning and robust zero-shot transfer for table-top manipulation.",OXE,CLIP image encoder (ViT-H-14),CLIP text encoder,Cosine similarity head over text-embedded motion primitives.,early,additive,autoregressive,manipulation,7.0,7.0,0.80154311312429,0.5212787393843652,3.8082944338692,-0.0326244558824293,0.2184845651381319,proxy_existing,0.3,0.12,0.7377718833747022,0.5375892445275858,0.12
Diffusion Policy,Yes,Yes,Introduced diffusion-based policy modeling for multimodal visuomotor action distributions.,Self-collected demos,ResNet-18,,diffusion policy network,late,diffusion,diffusion,manipulation,7.0,7.5,0.6004810936459812,0.6980436623114735,5.503005149695753,0.5318835886685056,0.7809547486005879,proxy_existing,0.3,0.12,0.3898637267015024,1.0,0.12
Octo,Yes,Yes,"First generalist diffusion policy trained on 4 M+ trajectories across 22 robot platforms, demonstrating broad transfer.",Open X-Embodiment,CNN encoder,T5-base,Diffusion Transformer head,late,diffusion,diffusion,manipulation,7.5,8.0,0.3227090520727584,0.8295345763188728,4.458287302375932,0.4658772778695578,0.7504740851197964,proxy_existing,0.3,0.8,0.7377718833747022,1.0,0.8
VLATest,No,Yes,"Automated framework for large-scale VLA model testing, revealing robustness gaps and guiding improvements in manipulation.",none,"that used in (e.g., OpenVLA,RT-1,Octo)","that used in (e.g., OpenVLA,RT-1,Octo)","that used in (e.g., OpenVLA,RT-1,Octo)",late,additive,autoregressive,manipulation,7.5,7.5,0.5545056088981004,0.4751191506623027,6.907711802091106,-0.1179774456929162,0.1093580731933826,proxy_existing,0.0,0.12,0.3898637267015024,0.452236254717099,0.12
NaVILA,Yes,No,Hierarchical planning yields 88% real-world navigation success for legged robots via language-conditioned topological control.,Self-collected Real-world legged robot demos,VILA Vision Encoder,VILA LLM,Topological graph planner + RL policy for joint commands,hierarch,additive,planner,navigation,7.5,7.5,0.7751527583293213,0.4385669899215464,4.086296272053481,0.4550345295367993,0.8403999999999999,paper_numeric,0.35,0.26,0.47881770215557,0.9896455387863037,0.32
RoboNurse-VLA,Yes,No,"Real-time voice-to-action pipeline for surgical instrument handover, robust to unseen tools in dynamic scenes.",Self-collected Surgical handover videos + voice prompts,SAM2,LLaMA-2,token-based action decoder,late,additive,mlp,manipulation,7.5,8.0,0.6717160069285915,0.7951312363070258,4.117906098910328,-0.0929537651493081,0.1590808361542939,proxy_existing,0.5,0.4,0.3898637267015024,0.4772599352607071,0.4
Mobility VLA,Yes,No,Multimodal instruction navigation with topological mapping for robust long-range mobility.,MINT instruction tours,Gemini 1.5 Pro based (ViT),Gemini 1.5 Pro based text encoder,Topological graph-based planner,hierarch,additive,planner,navigation,8.0,9.0,0.7263697730609899,0.4244581712173748,4.471255828659306,-0.0401300863488456,0.213256176021075,proxy_existing,0.35,0.06,0.47881770215557,0.4944809229006587,0.32
RevLA,Yes,No,Domain adaptation adapters to improve the generalization of robotic foundation models across visual domains.,Open X-Embodiment (OXE),DINO-v2 + SigLIP,LLama-7B,"Llama head, outputs 7 discrete action tokens",mid,additive,mlp,manipulation,8.0,8.0,0.7481729443270889,0.728190135386235,4.388740632873995,0.0352284522771958,0.281859914872038,proxy_existing,0.15,0.4,0.7377718833747022,0.605442152687211,0.4
Uni-NaVid,Yes,No,Video-based VLA unifying embodied navigation tasks across multiple benchmarks.,Room-to-Room (R2R) + REVERIE,EVA-CLIP,Vicuna-7B,Vicuna-7B head (4 discrete action tokens),mid,additive,mlp,navigation,7.0,8.0,0.5336895855899688,0.41801142183755,3.676526626155282,-0.1175401459611071,0.1098099991137112,proxy_existing,0.0,0.24,0.20145637098473557,0.41707086328839726,0.24
RDT-1B,Yes,Yes,1.2B-parameter diffusion foundation model excelling at bimanual manipulation and zero-shot generalization.,self-collected 6K ALOHA episodes,SigLIP,T5-XXL,Diffusion Transformer + MLP decoder,late,diffusion,diffusion,manipulation,8.0,8.0,0.6720586562785535,0.7670028742560422,5.147391144169445,0.5449403929944054,0.8785663095298731,proxy_existing,0.35,0.12,0.3898637267015024,1.0,0.12
RoboMamba,Yes,No,Mamba-based unified VLA with linear-time inference for real-time robotic reasoning.,SAPIEN sim benchmarks + real-world demos,Mamba VLM visual backbone,Mamba VLM text backbone,MLP policy head for SE(3) pose predicting,late,additive,mlp,manipulation,9.0,9.0,0.5989190983038765,0.7379112010598219,6.041328838185546,0.0706039411423014,0.304246865943765,proxy_existing,0.45,0.24,0.3898637267015024,0.6408176415523166,0.24
Chain-of-Affordance,No,Yes,"Sequential affordance reasoning for spatial planning, achieving SOTA on LIBERO dataset.",LIBERO + real/sim manipulation tasks,Qwen2-VL,Qwen2-VL,Diffusion policy head,late,diffusion,diffusion,manipulation,9.0,9.0,0.7785605096261032,0.5029683756647294,4.094399835877364,-0.0177831351170728,0.2392033920239655,proxy_existing,0.35,0.44,0.3898637267015024,0.5524305652929424,0.44
Edge VLA,Yes,Yes,"Lightweight, edge-optimized VLA for low-power real-time inference.",OXE + Bridge robotics set,SigLIP + DINOV2,Qwen2,Non-autoregressive control head,late,additive,autoregressive,manipulation,8.0,9.0,0.8094491324648676,0.802111645826587,4.244493746892606,0.275430868818484,0.5159245576157531,proxy_existing,0.35,0.24,0.7377718833747022,0.8456445692284992,0.24
OpenVLA,Yes,Yes,LORA-fine-tuned open-source VLA achieving efficient transfer and high success.,OXE + DROID robot data,DINOv2 + SigLIP,Llama 2,Llama 2 output head (predicts discretized action tokens as output),late,additive,mlp,manipulation,8.0,8.0,0.8710812584877896,0.777229394845665,3.9601407565225455,0.1550862384637974,0.3915548087374619,proxy_existing,0.0,0.24,0.7167665360534584,0.7252999388738126,0.24
CogACT,Yes,Yes,"Componentized diffusion action transformer, +59.1% success over OpenVLA with specialized adaptation.",OXE subset + real trials,DINOv2 + SigLIP,LLaMA-2,Diffusion Transformer head,late,diffusion,diffusion,manipulation,8.0,8.0,0.4683348176971607,0.6784157112355569,4.188797063286583,0.5986942506852001,0.85,proxy_existing,0.45,0.24,0.7377718833747022,1.0,0.24
ShowUI-2B,Yes,Yes,GUI/web navigation via screenshot grounding and efficient token selection.,256 K GUI instruction demos,Qwen2-VL-2B ViT,Qwen2-VL-2B LLM,Qwen2-VL-2B output head (GUI actions as tokens),late,additive,mlp,navigation,8.0,9.0,0.8105976816115041,0.8123353177128446,4.872360769407647,0.0790070604255219,0.3129310409831357,proxy_existing,0.0,0.12,0.3898637267015024,0.6136180696750262,0.12
Pi-0,Yes,No,"General robot control flow model for high-frequency, open-world tasks.",Extended OXE called Pi-Cross-Embodiment,PaliGemma (SigLIP),PaliGemma (Gemma-2B),diffusion-based Flow matching action expert head,late,diffusion,diffusion,manipulation,8.0,8.0,0.8615764604594311,0.7188576246242815,6.057246465169008,0.3782902147459149,0.6222241987802719,proxy_existing,0.4,0.12,0.7377718833747022,0.94850391515593,0.12
HiRT,Yes,Yes,"Hierarchical planning/control separation, doubling execution speed and improving dynamic task success.",Self collected Real-world data,InstructBLIP,LLaMA-2,Latent-conditioned policy head (MLP),hierarch,additive,mlp,manipulation,8.0,8.0,0.7424706261681373,0.6183283851437078,3.651017809805437,0.3866988128803661,0.7088504756510811,proxy_existing,0.45,0.32,0.3898637267015024,0.9569125132903813,0.32
A3VLM,No,Yes,"Learns articulation-aware affordance grounding purely from RGB video, generalizing to unseen object joints.",PartNet-Mobility,"CLIP, DINOv2, Q-Former (fused)",LLaMA2,Parameterized primitive motion generator,late,additive,autoregressive,navigation,7.0,8.0,0.5533097243367184,0.5264955956228664,6.042721227901822,0.0551252077585859,0.3187710544537268,proxy_existing,0.3,0.12,0.37685734356992767,0.5897362170080902,0.12
SVLR,No,Yes,"Modular ""segment-to-action"" pipeline using visual prompt retrieval for on-device policy execution.",Self-collected visual prompts,Mini InternVL,Phi-3mini4k,Script-based action binder,late,additive,autoregressive,manipulation,7.5,7.5,0.8933417004757387,0.8468568238188241,5.907632030245823,-0.0238427377497566,0.2066412669106012,proxy_existing,0.0,0.12,0.3898637267015024,0.5463709626602585,0.12
Bi-VLA,No,Yes,"Dual-arm instruction-to-action planner grounded in recipe demonstrations, achieving 83.4 % real-world task success.",Visual-recipe demos,Qwen-VL,Starling-LM,Python trajectory generator,hierarch,additive,autoregressive,manipulation,9.0,7.5,0.6106704700196163,0.4054942812353166,4.759690225340879,0.1379315483614064,0.7964699999999999,paper_numeric,0.35,0.32,0.3898637267015024,0.7081452487714216,0.32
QUAR-VLA,Yes,No,"Quadruped-specific VLA with adaptive gait and body command mapping, strong sim-to-real transfer.",QUART locomotion + manipulation,EfficientNet-B3,FiLM / VLM tokenizer,Transformer decoder (discrete tokens),late,additive,autoregressive,manipulation,7.0,7.5,0.8789528921693748,0.5517622731134857,6.378896212281935,-0.0875016671504077,0.1507957390644215,proxy_existing,0.2,0.24,0.8842033290299466,0.4827120332596075,0.24
3D-VLA,Yes,Yes,"Integrates 3D generative diffusion heads for world reconstruction, enabling planning in RGB+D and point-cloud spaces.",3D-language-action pairs,3D-aware transformer,3D-LLM,Multi-head diffusion planner,hierarch,diffusion,diffusion,manipulation,7.5,7.5,0.4384030747261169,0.7369067048032512,6.391823052221341,0.4038028464384491,0.7287100998409468,proxy_existing,0.35,0.32,0.47881770215557,0.9740165468484643,0.32
RoboMM,Yes,Yes,MIM-based multimodal decoder unifying 3D perception and language for spatially aligned policy fusion.,"RoboData (CALVIN, Meta-World)",Multi-view CLIP + occupancy network,Flamingo-style fusion module,Multimodal MLP/attention decoder,late,additive,mlp,manipulation,7.0,7.5,0.7528605569981606,0.4102926083537084,6.108774125748219,0.0547340770357985,0.287846208865495,proxy_existing,0.0,0.24,0.6806071012518389,0.6249477774458136,0.24
FAST,Yes,Yes,Frequency-space action tokenization for up to 15 times faster inference on general robot control.,DROID,PaliGemma (SigLIP),PaliGemma (Gemma-2B),FAST token generator,late,additive,mlp,manipulation,8.0,8.0,0.830380567853223,0.6192107215333876,5.878904525335356,0.073376766305158,0.3071124326763355,proxy_existing,0.0,0.12,0.4253789985146976,0.6435904667151732,0.12
OpenVLA-OFT,Yes,Yes,"Optimized fine-tuning of OpenVLA with parallel chunked decoding, achieving 97.1 % success on LIBERO dataset and 26 time speed-up.",LIBERO,SigLIP + DINOv2,LLaMA-27B,Llama 2 Parallel chunking head,late,additive,autoregressive,manipulation,8.0,8.0,0.4998374778745145,0.7382447093624558,3.0696131738583787,-0.1304647427412237,0.8253499999999999,paper_numeric,0.3,0.12,0.3898637267015024,0.4397489576687915,0.12
CoVLA,Yes,No,"VLA model for autonomous driving, trained on richly annotated scene data for robust planning.",Large-scale driving videos + annotations,CLIP ViT,LLaMA-2,Trajectory prediction module,late,additive,autoregressive,navigation,7.0,8.0,0.8573736882153011,0.8922056678402923,4.416574868354955,-0.106019946541838,0.1367509553716897,proxy_existing,0.35,0.44,0.3898637267015024,0.42859106270766634,0.44
ORION,Yes,No,Holistic end-to-end driving VLA aligning semantic understanding with generative trajectory control.,E2E driving benchmark,EVA-02-L (Transformer),Vicuna v1.5 (LoRA),Generative planner head,hierarch,additive,planner,navigation,7.5,8.0,0.3121116407057454,0.6835457887587044,3.1025491565174628,-0.0791272625806686,0.1679762564161479,proxy_existing,0.35,0.32,0.47881770215557,0.45548374666883573,0.32
UAV-VLA,Yes,No,Zero-shot aerial mission VLA combining satellite and UAV imagery for scalable instruction-driven flight planning.,Satellite + UAV flight logs,Molmo-7B-D (CLIP),GPT-3,Transformer-based path planner,hierarch,additive,autoregressive,navigation,7.0,7.5,0.7633185582803894,0.5459176459049047,4.163010960167183,0.2221872435769551,0.5666360227791672,proxy_existing,0.6499999999999999,0.44,0.47881770215557,0.7567982528264594,0.44
Combat VLA,Yes,No,"Ultra-fast tactical reasoning in 3D ARPG environments, achieving 50 times faster inference and human-level success.",3D ARPG combat logs,Qwen2.5-VL-3B,Qwen2.5-VL-3B,LLM-based planner head,hierarch,additive,planner,manipulation,9.0,9.0,0.8722463858394431,0.7691423192688649,6.314340520774204,-0.0410770975007819,0.225485804231398,proxy_existing,0.55,0.32,0.47881770215557,0.5291366029092333,0.32
HybridVLA,Yes,No,Adaptive ensemble decoding that combines diffusion and autoregressive policies for robust multi-task generalization.,RT-X trajectories + synthetic task fusion,CLIP ViT + DINOV2,LLaMA-2,Diffusion policy head,late,diffusion,diffusion,manipulation,7.0,8.0,0.6050528944718216,0.461431006893817,5.769134126612075,0.2864995796461315,0.5273634791694917,proxy_existing,0.4,0.24,0.3898637267015024,0.8567132800561467,0.24
NORA,Yes,No,Low-overhead VLA with integrated visual reasoning and FAST token decoding for real-time performance.,OXE,Qwen-2.5-VL,Qwen-2.5-VL,FAST tokenizer head,late,additive,mlp,manipulation,9.0,9.0,0.3313439391961579,0.7295266372278755,5.418017977476206,0.1491638344176242,0.3854343204214691,proxy_existing,0.0,0.12,0.7377718833747022,0.7193775348276394,0.12
SpatialVLA,Yes,No,3D spatial encoding and adaptive action discretization to improve cross-robot manipulation generality.,OXE,SigLIP,PaliGemma (Gemma-2B),Adaptive action grid head,mid,additive,autoregressive,manipulation,8.0,8.0,0.7828334685797784,0.5129886648940307,3.2933772307593845,0.0140378213852738,0.2457888018949863,proxy_existing,0.0,0.12,0.7377718833747022,0.5842515217952889,0.12
MoLe-VLA,Yes,No,Selective layer activation in a multi-stage ViT yields 5.6 time faster inference and +8% task success.,RLBench + real-world trials,"DINOv2, SigLIP",LLaMA-2,Diffusion head,late,diffusion,diffusion,manipulation,8.0,8.0,0.3381384853626232,0.7531654120903784,5.878160849259938,0.0668179041229193,0.3003341988645883,proxy_existing,0.0,0.24,0.3015061411624024,0.6370316045329345,0.24
JARVIS-VLA,Yes,No,Open-world instruction following in 3D games via keyboard/mouse action prediction.,Minecraft gameplay demos,ViT (in Llava-Next/Qwen2-VL),Llava-Next/Qwen2-VL (transformer LLMs),Key/mouse control head,late,additive,autoregressive,manipulation,8.0,9.0,0.6361693899749707,0.8701695243781493,3.408414137057175,0.0385793607747988,0.2711511722971074,proxy_existing,0.0,0.12,0.3898637267015024,0.608793061184814,0.12
UP-VLA,Yes,No,"Precise 3D spatial reasoning, achieving +33 % success on the CALVIN benchmark.",CALVIN,CLIP-ViT,Phi-1.5,MLP policy head,late,additive,mlp,manipulation,7.0,7.5,0.5181920159644303,0.6334131185569574,4.196988220821242,-0.0902076214084636,0.1380567036389008,proxy_existing,0.0,0.12,0.6806071012518389,0.48000607900155157,0.12
Shake-VLA,Yes,Yes,Modular bimanual VLA achieving 100% success on cluttered cocktail-mixing tasks.,Cocktail mixing demos,"SigLip","GPT-4o,Whisper-1",Bimanual arm controller,late,additive,autoregressive,humanoid,7.5,7.5,0.4448401886745534,0.6276009231406356,6.762219965790784,-0.0394644513160593,0.91,paper_numeric,0.3,0.12,0.2898637267015024,0.5392395166202284,0.12
MORE,No,No,Scalable Mixture of Expert (MoE) enhanced RL framework for quadruped multi-task learning.,Quadruped navigation/manipulation demos,CLIP-like,Fuyu 8B,Mixture-of-Experts + LoRA adapter,mid,additive,autoregressive,manipulation,7.0,7.5,0.5959248111223034,0.540357107950767,6.389300338627418,0.046854279628349,0.2797028590385192,proxy_existing,0.0,0.12,0.3898637267015024,0.6170679800383642,0.12
DexGraspVLA,Yes,No,Diffusion-based dexterous grasping with $\geq90\%$ zero-shot success across diverse objects.,Self-collected Dexterous grasp data,DINOv2,"Qwen-VL, Qwen2.5-VL",Diffusion policy head,late,diffusion,diffusion,manipulation,8.0,9.0,0.6385237394884777,0.7751252151945127,4.630556554737348,0.352775558805779,0.846,paper_numeric,0.6,0.12,0.3898637267015024,0.9229892592157942,0.12
DexVLA,Yes,No,Cross-embodiment diffusion expert enabling rapid adaptation without per-task tuning.,"OXE, RLBench","Qwen2-VL (ViT), ResNet-50","Qwen2-VL,DistilBERT",Diffusion Transformer head,late,diffusion,diffusion,manipulation,7.0,9.0,0.5241957388462013,0.7614303867871333,4.111684899578842,0.4168179284148171,0.6620405334163396,proxy_existing,0.35,0.24,0.6747065794839875,0.9870316288248322,0.24
Humanoid-VLA,Yes,No,"Hierarchical VLA for full-body humanoid control, integrating perception and latent action planning.",Self-collected humanoid robot episodes,"Video Visual Encoder,Cross-Attention",Llama3-70B,Token-based Motion Decoder + RL Whole-Body Ctrlr,hierarch,additive,mlp,humanoid,7.5,8.0,0.7293618801983466,0.5517167898798925,4.90020348437308,0.3507757032082798,0.6671398782416281,proxy_existing,0.45,0.32,0.2898637267015024,0.9294796711445675,0.32
ObjectVLA,Yes,No,End-to-end open-world object manipulation without task-specific data.,RoboSpatial manipulation episodes,"DinoX, DiVLA VLM",DiVLA (LLM backbone),Object-centric diffusion controller head,late,diffusion,diffusion,manipulation,8.0,7.5,0.7501297951697132,0.7995927374636935,4.197893861658466,-0.1401397338299706,0.0864545319049163,proxy_existing,0.0,0.12,0.3898637267015024,0.4300739665800446,0.12
Gemini Robotics,Yes,Yes,"General-purpose VLA built on the Gemini 2.0 foundation, enabling long-horizon dexterous manipulation across diverse robot embodiments with zero-shot adaptability.",Self-collected ALOHA2 demos + web-scale VL Dataset,Gemini 2.0 vision component,Gemini 2.0 language component,Local zero-shot policy head,late,additive,mlp,manipulation,9.0,9.0,0.5024443708913685,0.4136220004578811,5.073136838466522,0.1122884523933304,0.4453939820120637,proxy_existing,0.7999999999999999,0.8400000000000001,0.3898637267015024,0.6825021528033456,0.8400000000000001
ECoT,Yes,Yes,"Embodied chain-of-thought planning for interpretable, stepwise VLA control.",Bridge v2,"SigLIP, DINOv2",LLaMA-2 7B,Autoregressive VLA decoder with CoT module,hierarch,additive,autoregressive,manipulation,8.0,8.0,0.6121139121150764,0.4207910090872726,5.305822709931176,-0.163697706863153,0.0697808666961356,proxy_existing,0.35,0.32,0.3898637267015024,0.4065159935468622,0.32
OTTER,Yes,Yes,Zero-shot generalization via a frozen CLIP backbone and causal transformer action decoding.,LIBERO,Frozen CLIP ViT,CLIP text encoder,Causal transformer delta-trajectory head,early,additive,autoregressive,manipulation,7.0,7.0,0.3873813757354211,0.4030068262562456,3.787310338808125,0.0114451996703417,0.2688504680641592,proxy_existing,0.3,0.12,0.3898637267015024,0.5816589000803569,0.12
OneTwoVLA,Yes,Yes,Unified reasoning-acting framework that dynamically toggles between planning and control via decision tokens.,Self-collected 16K reasoning-augmented robot episodes,same as pi-0 vla,same as pi-0 vla,Diffusion policy head,late,diffusion,diffusion,manipulation,7.5,7.5,0.6857394471873495,0.4018821796952228,6.351023520346339,-0.1212285745536378,0.1190921010592182,proxy_existing,0.35,0.32,0.3898637267015024,0.4489851258563774,0.32
Helix,Yes,Yes,"First 200 Hz VLA for full humanoid control on embedded systems, enabling zero-shot task transfer.",self-collected 200Hz teleop + sim logs,Pretrained VLM,Pretrained VLM,Fast transformer policy,hierarch,additive,autoregressive,humanoid,7.5,7.5,0.4485294028643533,0.8468204737367604,4.251979757421267,0.0445625847673235,0.3409583152292508,proxy_existing,0.6499999999999999,0.44,0.2898637267015024,0.6232665527036112,0.44
Gemini Robotics On-Device,Yes,Yes,"On-device optimized variant of Gemini VLA, delivering low-latency dual-arm and humanoid control on embedded hardware.",Self-collected ALOHA2 + few-shot adaptation demos,Gemini SDK vision module,Gemini SDK language module,On-device optimized policy head,hierarch,additive,mlp,manipulation,9.0,9.0,0.6737207967711781,0.7512978084654224,4.58960465341014,0.0578192683198506,0.3269859209233182,proxy_existing,0.35,0.44,0.3898637267015024,0.6280329687298658,0.44
OE-VLA,Yes,Yes,Curriculum-tuned LLaVA backbone with interleaved multimodal prompting for improved generalization across vision-language-action tasks.,CALVIN,SigLIP-400M ViT,Qwen-1.5 language module,MLP token generator,mid,additive,mlp,manipulation,8.0,9.0,0.4023315440944844,0.5080506459769691,5.5938358251813005,-0.0791967599127408,0.149435840997341,proxy_existing,0.0,0.12,0.6806071012518389,0.4910169404972744,0.12
SmolVLA,Yes,Yes,"Ultra-lightweight VLA trained on community-contributed robot demonstrations, capable of real-time inference on CPU.",22.9K community episodes,SigLIP (VLM-2) visual backbone,SmolVLM2 text backbone,Chunked flow-matching head,late,flow,flow,manipulation,8.0,7.5,0.4471432034565596,0.563694615968473,5.88563264010242,0.159064194319152,0.3956658136848474,proxy_existing,0.0,0.12,0.3898637267015024,0.7292778947291672,0.12
EF-VLA,Yes,Yes,"Early fusion of fine-grained CLIP visual tokens into the language-action pipeline, boosting zero-shot generalization.",Self-collected real and simulated tasks,Frozen CLIP ViT,Frozen CLIP text encoder,causal transformer,early,additive,autoregressive,manipulation,7.0,7.0,0.5388591101212976,0.4685851285083851,3.5869546434921373,0.0809525659268737,0.348288376475499,proxy_existing,0.3,0.24,0.3898637267015024,0.6511662663368889,0.24
PD-VLA,Yes,No,"First parallel decoding method with action chunking for VLA, achieving a 2.52 times speed-up without sacrificing control fidelity.",Chunked trajectory demonstrations,CLIP-ViT-Large-Patch14-336 (LLaVA),Vicuna-7B-v1.5 (LLaVA),Fixed-point token predictor,late,additive,mlp,manipulation,7.0,8.0,0.7531608497915556,0.5928706949108107,5.624121359444812,0.0230567439223281,0.2551093767123281,proxy_existing,0.0,0.12,0.3898637267015024,0.5932704443323433,0.12
LeVERB,Yes,Yes,"Dual-process latent VLA for whole-body humanoid control, achieving 58.5 % success on sim-to-real humanoid demos.",sim-to-real humanoid demos,SigLIP ViT,SigLIP text encoder,Latent CVAE verb + transformer policy,hierarch,additive,autoregressive,humanoid,8.0,8.0,0.4677651620303111,0.8752518873022228,5.220993618710439,0.0662573231807932,0.593775,paper_numeric,0.55,0.32,0.2898637267015024,0.644961291117081,0.32
TLA,Yes,Yes,"First language-grounded tactile-action model for high-precision contact tasks, with 85 % success on peg-in-hole task.",TLA Data,ViT (Qwen2-VL),Qwen2-VL,Multimodal $,late,additive,autoregressive,manipulation,8.0,9.0,0.7669484351875,0.4149663161823027,4.457390751517612,0.03072178458818,0.7224999999999999,paper_numeric,0.45,0.12,0.3898637267015024,0.6009354849981952,0.12
Interleave-VLA,Yes,Yes,Model-agnostic wrapper enabling interleaved image-text instruction processing..,Interleave-VLA data,"Any base VLM (e.g., OpenVLA)","Any LLM (e.g., Pi-0)",Minimal interleaved-processing module,late,additive,autoregressive,manipulation,7.5,7.5,0.6303845198807647,0.4537009977663317,3.422382721616545,0.1130894948373251,0.3481534168919227,proxy_existing,0.0,0.12,0.5628722799994196,0.6833031952473403,0.12
iRe-VLA,Yes,Yes,Iterative RL and supervised fine-tuning pipeline for robust control and rapid generalization across embodiments.,"Franka-Kitchen, real Panda robot demos",BLIP-2 (pre-trained VLM),BLIP-2,MLP action head after token learner,mid,additive,mlp,manipulation,8.0,8.0,0.5783937533742227,0.5723963531739436,3.638683175672992,0.1105055446043245,0.3454830423131388,proxy_existing,0.0,0.4,0.3898637267015024,0.6807192450143397,0.4
TraceVLA,Yes,Yes,"Visual trace prompting to incorporate spatio-temporal cues, boosting task success by 3.5 time over OpenVLA.",OXE + 150K trace-annotated demos,Phi-3-Vision with trace overlay,Phi-3 LLM,Quantized delta-motion tokens,late,additive,mlp,manipulation,7.5,7.5,0.8747681572690535,0.5723812407690863,3.399380958916897,-0.0145488699954366,0.2162459829274784,proxy_existing,0.0,0.24,0.7377718833747022,0.5556648304145786,0.24
OpenDrive VLA,Yes,No,End-to-end driving VLA with semantic scene alignment and temporal abstraction for robust trajectory planning.,Autonomous driving QA/planning benchmarks,ResNet-101 + Query Transformers,Qwen2.5-Instruct (LLM),Ego-vehicle action autoregressor,late,additive,autoregressive,navigation,7.0,9.0,0.5998688384267921,0.6852146388882289,6.923201500103629,0.1016760412123407,0.3779083553810981,proxy_existing,0.35,0.32,0.3898637267015024,0.636287050461845,0.32
V-JEPA 2,Yes,No,Dual-stream self-supervised video JEPA enabling predictive planning in vision-language-action tasks.,Droid video data,ViT (self-supervised),LLM for QA/alignment,Action-conditioned transformer predictor head,late,additive,autoregressive,manipulation,8.0,7.5,0.7273879472297227,0.78388197737624,4.94189810601112,0.023234551454188,0.2868293416385221,proxy_existing,0.35,0.32,0.4253789985146976,0.5934482518642031,0.32
Knowledge Insulating VLA,Yes,No,"Implements insulation layers between vision-language and action modules, accelerating training and inference while maintaining generalization.",Multi-domain VL datasets,PaliGemma (SigLIP),PaliGemma (Gemma-2B) encoder,Diffusion Modular policy head,late,diffusion,diffusion,manipulation,8.0,8.0,0.8562742460021622,0.701619200843473,4.50872085098678,0.1539494004845021,0.3903799473948033,proxy_existing,0.0,0.12,0.3898637267015024,0.7241631008945173,0.12
GR00T N1,Yes,No,Self-collected Diffusion-based foundation model enabling unified humanoid control with policy tokenization.,Multi-modal humanoid demonstrations, Eagle-2 VLM,SmolLM2 (Eagle-2 VLM),Generative diffusion transformer based planner,hierarch,diffusion,diffusion,humanoid,8.0,9.0,0.8496329592839016,0.7549440183147114,4.811012121791057,0.4343490004782238,0.7641774729383877,proxy_existing,0.45,0.32,0.37881770215557004,1.0,0.32
AgiBot World Colosseo,Yes,No,Integrates multiple embodied datasets into a unified platform for scalable training and evaluation of VLA models.,AgiBot World Data,PaliGemma (SigLIP),PaliGemma (Gemma-2B),Latent action planner + policy head,hierarch,additive,mlp,manipulation,8.0,8.0,0.7626189584378686,0.6755677872681651,6.665700127592268,0.0166972498376189,0.2792388307289179,proxy_existing,0.35,0.32,0.47881770215557,0.5869109502476341,0.32
Hi Robot,Yes,No,Hierarchical separation of planning and control for open-ended instruction following in complex environments.,Self-collected Instruction-following data,PaliGemma-3B (SigLIP),PaliGemma-3B (Gemma-2B),Flow-Matching Action Expert,hierarch,flow,flow,manipulation,8.0,8.0,0.58582712374957,0.6379285578026657,4.897091309085307,0.2584483445529367,0.595117218901334,proxy_existing,0.55,0.32,0.3898637267015024,0.8286620449629518,0.32
EnerVerse,Yes,No,"World-model LLM for predictive future-space modeling, enabling long-horizon manipulation planning.",self-collected Synthetic task fusion data,Pretrained VAE + Diffusion Generator,Tokenized instruction prompt,Diffusion Policy Head,late,diffusion,diffusion,manipulation,7.5,7.5,0.5328958801276293,0.6145952370943397,5.688954969783967,0.177573106395872,0.4660330182882503,proxy_existing,0.35,0.32,0.3898637267015024,0.7477868068058872,0.32
FLaRe,Yes,No,"Large-scale RL fine-tuning framework generating robust, adaptive robot policies across domains.",Multi-domain RL demonstrations,DinoV2,Transformer policy (language tokens),RL policy head,late,additive,mlp,manipulation,8.0,7.5,0.3776007567066373,0.880133513833798,6.637642281000272,-0.0427204702940124,0.1871321384268374,proxy_existing,0.0,0.12,0.3898637267015024,0.5274932301160028,0.12
Beyond Sight,Yes,No,Fuses heterogeneous sensor modalities via language-grounded attention to improve VLA generalization.,self-collected Multi-sensor data,Multi-modal ViT,"Transformer (shared, task language input)",Transformer action head,late,additive,autoregressive,manipulation,8.0,7.5,0.5914017715629405,0.5134154999230425,4.789740088954402,-0.223796240162281,0.0,proxy_existing,0.0,0.12,0.3898637267015024,0.3464174602477342,0.12
GeoManip,Yes,No,"Encodes geometric constraints as model interfaces, enhancing robustness and precision in manipulation.",Self-collected Simulated geometry tasks,VLM (GPT-4o) + Grounding-DINO,GPT-4o,Constraint solver head,late,additive,autoregressive,manipulation,8.0,7.5,0.8592215756878294,0.7057043991085642,5.49446633883572,-0.0042860136644593,0.226852096892811,proxy_existing,0.0,0.12,0.3898637267015024,0.5659276867455558,0.12
Universal Actions,Yes,No,Defines a universal action dictionary to standardize policy transfer and improve cross-task adaptability.,Self-collected Cross-domain manipulation demos,Shared VLM (LLaVA-OneVion-0.5B),LLaVA,Unified action tokenizer head,mid,additive,mlp,manipulation,7.5,7.5,0.5080775514611926,0.7479414564288672,6.123886852408592,0.049215753645793,0.2821433163291065,proxy_existing,0.0,0.12,0.3898637267015024,0.6194294540558082,0.12
RoboHorizon,Yes,No,LLM-enhanced multi-view environment modeling for robust long-horizon task planning.,Self-collected Multi-view robot trajectories,Multi-view transformer (ViT),GPT‐based planner,DreamerV2 Actor-Critic RL Head,late,additive,autoregressive,manipulation,8.0,7.5,0.358775347039228,0.844277061541385,3.855277454890607,-0.0192404348836518,0.2375113101183754,proxy_existing,0.35,0.32,0.3898637267015024,0.5509732655263634,0.32
SAM2Act,Yes,No,Utilizes SAM-based segmentation prompts with memory-augmented VLA for improved object-centric manipulation.,SAM-labeled manipulation tasks,SAM2 segmentation encoder,CLIP text encoder,Memory-augmented policy head,late,additive,mlp,manipulation,7.5,7.0,0.3454384014982286,0.4094844458593771,6.974959026767888,-0.0011549529406109,0.2300878809473218,proxy_existing,0.0,0.12,0.3898637267015024,0.5690587474694043,0.12
LMM Planner Integration,Yes,No,Merges LMM-based strategic planning with 3D skill policies for generalizable manipulation.,skill library demos,DINO (2D semantics) + PointNext (3D),CLIP Language Encoder,3D Transformer head,late,additive,autoregressive,manipulation,8.0,7.0,0.4179610818764548,0.8514658040981538,6.7964056674593,-0.0368246936538394,0.2170940927615814,proxy_existing,0.35,0.32,0.3898637267015024,0.5333890067561757,0.32
VLA-Cache,Yes,No,"Introduces token-caching to reuse computation across time steps, boosting inference efficiency.",LIBERO,CLIP ViT,LLaMA-2,Cached inference head,mid,additive,autoregressive,manipulation,7.0,8.0,0.6572118829346982,0.77285784951399,6.574836967372796,0.0654751969282156,0.2989465826815463,proxy_existing,0.0,0.12,0.3898637267015024,0.6356888973382308,0.12
Forethought VLA,Yes,No,Aligns latent vision and action spaces for foresight-driven policy steering.,Self-collected Latent alignment demonstrations,Phi-3 Vision,LLama,Diffusion policy,late,diffusion,diffusion,navigation,7.5,8.0,0.7392750350309402,0.8691510579125932,5.560156438272259,-0.0797009055556669,0.1489148333975502,proxy_existing,0.0,0.3,0.3898637267015024,0.45491010369383744,0.12
GRAPE,Yes,No,Preference-guided policy adaptation via personalized feedback alignment.,Self-collected Preference-labeled demos,Dinov2,LLaMA-2,Autoregressive transformer head,late,additive,autoregressive,gui,8.0,8.0,0.7657873073703032,0.7198561215834023,4.298573254942454,-0.2123909698133598,0.0117867378461652,proxy_existing,0.0,0.12,0.3898637267015024,-0.2123909698133598,0.12
HAMSTER,Yes,No,Hierarchical skill decomposition to sequence multi-step manipulation actions.,Self-collected Decomposed manipulation tasks,VILA-1.5-13B,VILA-1.5-13B,Robotic View Transformer Skill execution head,hierarch,additive,autoregressive,manipulation,7.5,7.5,0.5872725958857852,0.6122794789388659,5.155750814495258,0.274606334705625,0.5786990418678443,proxy_existing,0.4,0.32,0.3898637267015024,0.8448200351156402,0.32
TempoRep VLA,Yes,No,Use successor representation temporal encoding for compositional action planning.,Self-collected Temporal demonstration sequences,ResNet-34 CNN,retrained transformer (CLIP-style),MLP (3x256) head on ResNet feature,late,additive,mlp,manipulation,7.0,7.0,0.3710916043128029,0.8319275862941342,5.733365924994738,-0.1718310310918341,0.0603372017239277,proxy_existing,0.35,0.32,0.3898637267015024,0.3983826693181811,0.32
ConRFT,Yes,No,Applies consistency regularized fine-tuning with reinforcement for stable policy learning.,Self-collected data for fine-tuning,same as in octo,same as in octo,Reinforced policy head,late,additive,mlp,manipulation,7.5,7.5,0.4512695045083348,0.4605643162990613,5.266195097623432,-0.20518428655428,0.0192344601461592,proxy_existing,0.0,0.12,0.3898637267015024,0.3650294138557352,0.12
RoboBERT,Yes,No,"Unified multimodal Transformer for end-to-end vision-language-action manipulation, pre-trained on diverse robot and language data.",Self-collected Multi-domain robot demos,CLIP ViT,BERT-base,CNN-based Diffusion Policy Head,late,diffusion,diffusion,manipulation,7.0,7.5,0.73429447311975,0.5937061404903248,3.310947712946261,0.1093259225687766,0.3442639659330793,proxy_existing,0.0,0.12,0.3898637267015024,0.6795396229787918,0.12
Diffusion Transformer Policy,Yes,No,"Adapts diffusion-based transformer architectures to VLA policy learning, enabling robust multimodal action sampling.",LIBERO + CALVIN,DINOv2,CLIP Text Encoder,Diffusion generator head,late,diffusion,diffusion,manipulation,8.0,7.0,0.586296483646221,0.5789505084739845,3.1318136039628754,0.4095749160585827,0.6545552669344674,proxy_existing,0.3,0.24,0.6806071012518389,0.9797886164685978,0.24
GEVRM,Yes,No,Generative video modeling of goal-oriented tasks to enhance planning for visual manipulation.,CALVIN,ResNet-34,T5 Encoder,Diffusion Policy,late,diffusion,diffusion,manipulation,7.0,8.0,0.6167495034897343,0.8037992235722431,3.906151656571228,-0.0823315495698903,0.1642557342839666,proxy_existing,0.35,0.32,0.6806071012518389,0.4878821508401249,0.32
SoFar,Yes,No,Introduces successor-feature orientation representations bridging spatial reasoning and robotic manipulation.,Self-collected Orientation task demonstrations,"Florence-2 (ViT-style), SAM",CLIP Text Encode,"VLM (e.g., LLaVA or GPT-4o) for 6D goal pose, then motion planner",hierarch,additive,planner,manipulation,8.0,7.0,0.3489170139299367,0.8896439013460229,5.111842673628743,0.0156524486050095,0.2780257040262446,proxy_existing,0.35,0.32,0.47881770215557,0.5858661490150247,0.21
ARM4R,Yes,No,Auto-regressive 4D transition model for predicting and planning manipulator trajectories.,76K videos from the Epic-Kitchens100 dataset,ViT-Base,CLIP text encoder,2-layer MLP,late,additive,mlp,manipulation,8.0,7.0,0.8529551794647776,0.6817319807119595,4.234840949051182,0.0130965168287242,0.2750579920909524,proxy_existing,0.35,0.32,0.3898637267015024,0.5833102172387393,0.32
Magma,Yes,No,"Foundation multimodal agent model unifying vision, language, and action domains for end-to-end control.",Self-collected Multimodal interaction dataset,ConvNeXt-XXlarge,LLaMA-3-8B (decoder-only LLM),Decoder-Only LLM Head (LLaMA-3-8B),late,additive,autoregressive,manipulation,7.5,8.0,0.6613608069549721,0.6110504529653061,4.490546076315089,0.1325030261465944,0.3682162647868279,proxy_existing,0.0,0.12,0.3898637267015024,0.7027167265566095,0.12
An Atomic Skill Library,Yes,No,"Constructs an atomic skill library for modular, data-efficient composition of robotic actions.",Self-collected Skill primitive demonstrations,"Prismatic VLM (scene description.), DINO-X (obj detection), SAM-2 (segmentation)","Prismatic, GPT-4 (for planning)",Skill executor module,late,additive,autoregressive,manipulation,8.0,7.5,0.368608015236915,0.8657175772923327,3.2336294870169024,0.1079079756737837,0.3427985935377072,proxy_existing,0.0,0.12,0.3898637267015024,0.6781216760837989,0.12
VLAS,Yes,No,Integrates speech-based LLM guidance for customizable voice-driven vision-language-action control.,Speech-guided robot demos,CLIP ViT,Vicuna (LLaMA-7B/13B),Vicuna as an Autoregressive Action Decoder,late,additive,autoregressive,navigation,7.0,8.0,0.3757432183736746,0.5765497599066882,4.305284011899156,-0.1882637133958054,0.0367209689201196,proxy_existing,0.0,0.12,0.3898637267015024,0.34634729585369894,0.12
ChatVLA,Yes,Yes,Unified conversational VLA enabling natural-language and vision-driven interactive robot control with real-time multimodal feedback.,Interactive human-robot demos,ViT + LoRA,Qwen2-VL-2B (LLM),"mixture-of-expert action head, as in DiVLA)",late,additive,autoregressive,navigation,8.0,9.0,0.7836859506435703,0.5817369563660673,4.897984928779357,-0.0827524680580046,0.14576120650964,proxy_existing,0.0,0.12,0.3898637267015024,0.4518585411914997,0.12
RoboBrain,Yes,No,Knowledge-grounded policy brain that maps abstract high-level plans to concrete multimodal actions across diverse tasks.,Multi-domain robot and plan data,SigLIPr,Qwen2.5-7B-Instruct (decoder-only LLM),LoRA adapters for skill,mid,additive,autoregressive,manipulation,8.0,9.0,0.724751161058266,0.7818470241237657,5.372459221373023,-0.0810376702748838,0.1475333584455862,proxy_existing,0.0,0.24,0.3898637267015024,0.4891760301351314,0.24
SafeVLA,Yes,No,"Safety-aware VLA integrating constraint feedback through safe RL to ensure collision-free, reliable manipulation.",Safety-scenario demonstrations,"Modular (DINOv2, SigLIP, CLIP)","LLM (model-agnostic, e.g., T5, LLaMA, Qwen)",Safety-constraint policy head,late,additive,mlp,manipulation,7.0,8.0,0.62882291172083,0.606375032211623,4.553138862416713,-0.2176644525965327,0.0063368750020631,proxy_existing,0.0,0.12,0.3898637267015024,0.3525492478134825,0.12
CognitiveDrone,Yes,No,"Embodied cognitive reasoning VLA for UAVs, combining vision-language understanding with autonomous flight planning.",UAV mission logs,OpenVLA visual encoder	(ViT-style),OpenVLA’s language encoder,Transformer policy head,late,additive,autoregressive,navigation,8.0,7.5,0.6091492468742896,0.4337649563436088,5.294428731929651,-0.0194386743118218,0.2372811328020306,proxy_existing,0.35,0.32,0.3898637267015024,0.5151723349376826,0.32
Diffusion-VLA,Yes,Yes,"Multimodal VLA framework unifying vision-language reasoning with diffusion-based policy for robust, generalizable manipulation across diverse robot embodiments.",Multi-embodiment manipulation suites,SigLIP,Qwen2-VL (2B/7B/72B),Latent diffusion policy head + MLP,late,diffusion,diffusion,manipulation,8.0,9.0,0.3855532100040863,0.881294561592872,3.682033560676781,0.3702238675017123,0.6463880157586654,proxy_existing,0.45,0.52,0.3898637267015024,0.9404375679117275,0.52
