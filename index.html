<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multimodal Fusion with Vision-Language-Action Models for Robotic Manipulation: A Systematic Review</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #fff;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            text-align: center;
            margin-bottom: 15px;
            padding: 20px 0;
            background: #4169E1;
            color: white;
            border-radius: 10px;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 20px;
            font-weight: 700;
        }

        .subtitle {
            font-size: 1.2rem;
            color: #666;
            width: 100%;
            margin: 0 0 20px 0;
            text-align: left;
            font-weight: 500;
        }

        nav {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 40px;
            border-left: 4px solid #667eea;
        }

        nav h2 {
            margin-bottom: 15px;
            color: #333;
            font-size: 1.3rem;
        }

        nav ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            gap: 5px;
            justify-content: flex-start;
        }

        nav a {
            color: #667eea;
            text-decoration: none;
            font-weight: 500;
            padding: 6px 16px;
            border-radius: 4px;
            transition: all 0.3s ease;
            border-left: 4px solid transparent;
            display: inline-block;
            margin-right: 10px;
            font-size: 1.1rem;
            white-space: nowrap;
        }

        nav a:hover {
            background-color: #e9ecef;
            border-left: 4px solid #667eea;
        }

        section {
            margin-bottom: 50px;
        }

        h2 {
            font-size: 2rem;
            margin-bottom: 20px;
            color: #2c3e50;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }

        h3 {
            font-size: 1.5rem;
            margin-bottom: 15px;
            color: #34495e;
        }

        h4 {
            font-size: 1.2rem;
            margin-bottom: 10px;
            color: #34495e;
        }

        p {
            margin-bottom: 15px;
            text-align: justify;
        }

        .intro-points {
            display: flex;
            flex-wrap: wrap;
            gap: 12px;
            margin: 20px 0;
        }

        .point-card {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: 12px 16px;
            border-radius: 6px;
            border-left: 4px solid #667eea;
            transition: transform 0.2s, shadow 0.2s;
            flex: 1;
            min-width: 200px;
            max-width: 300px;
        }

        .point-card:hover {
            transform: translateY(-1px);
            box-shadow: 0 2px 8px rgba(102, 126, 234, 0.15);
        }

        .point-card h4 {
            color: #667eea;
            margin-bottom: 6px;
            font-size: 1rem;
        }

        .point-card p {
            margin: 0;
            font-size: 0.9rem;
            line-height: 1.4;
        }

        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 20px 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .code-block {
            background-color: #f6f8fa;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 16px;
            margin: 16px 0;
            overflow-x: auto;
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            font-size: 14px;
        }

        .models-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(400px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }

        .model-link {
            display: block;
            color: #0366d6;
            text-decoration: none;
            padding: 8px;
            border-radius: 4px;
            transition: background-color 0.2s;
            font-size: 0.95rem;
        }

        .model-link:hover {
            background-color: #f1f8ff;
        }

        .year-badge {
            background-color: #667eea;
            color: white;
            padding: 2px 6px;
            border-radius: 3px;
            font-size: 0.8rem;
            margin-right: 8px;
            font-weight: 600;
        }

        .analysis-features {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .feature-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }

        .feature-card h4 {
            color: white;
            margin-bottom: 10px;
        }

        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .metric-item {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 6px;
            border-left: 3px solid #28a745;
        }

        .metric-item strong {
            color: #28a745;
        }

        .plot-descriptions {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 25px;
            margin: 30px 0;
        }

        .plot-card {
            background: white;
            border: 1px solid #e1e4e8;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .plot-card h4 {
            color: #667eea;
            margin-bottom: 15px;
            font-size: 1.1rem;
        }

        .plot-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 20px;
            margin: 30px 0;
            align-items: stretch;
        }

        .plot-item {
            text-align: center;
            background: white;
            border-radius: 8px;
            padding: 15px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            transition: transform 0.3s ease;
            display: flex;
            flex-direction: column;
            height: 100%;
        }

        .plot-item:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }

        .plot-item img {
            width: 100%;
            height: 200px;
            object-fit: contain;
            border-radius: 6px;
            margin-bottom: 10px;
            background-color: #f8f9fa;
        }

        .plot-item h4 {
            margin: 10px 0 8px 0;
            color: #667eea;
            font-size: 1rem;
            flex-grow: 0;
        }

        .plot-item p {
            font-size: 0.85rem;
            color: #666;
            margin: 0;
            line-height: 1.4;
            flex-grow: 1;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        @media (max-width: 1024px) {
            .plot-grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }

        @media (max-width: 768px) {
            .plot-grid {
                grid-template-columns: 1fr;
            }
        }

        .citation-box {
            background-color: #f6f8fa;
            border: 1px solid #d0d7de;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            font-size: 14px;
            overflow-x: auto;
        }

        .menu-bar {
            background: #4169E1;
            padding: 5px 0;
            text-align: center;
            box-shadow: 0 2px 10px rgba(65, 105, 225, 0.2);
            border-radius: 15px;
            margin-bottom: 25px;
            width: 100%;
        }

        .menu-bar a {
            color: white;
            text-decoration: none;
            padding: 10px 20px;
            margin: 0 15px;
            border-radius: 25px;
            transition: all 0.3s ease;
            font-weight: 600;
            display: inline-block;
        }

        .menu-bar a:hover {
            background-color: rgba(255, 255, 255, 0.2);
            transform: translateY(-2px);
        }

        footer {
            text-align: center;
            padding: 40px 0;
            border-top: 1px solid #e1e4e8;
            margin-top: 60px;
            color: #666;
        }

        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }
            
            h1 {
                font-size: 2rem;
            }
            
            .intro-points {
                grid-template-columns: 1fr;
            }
            
            .models-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Multimodal Fusion with Vision-Language-Action Models for Robotic Manipulation: A Systematic Review</h1>
            <p style="font-size: 1.2rem; color: rgba(255, 255, 255, 0.95); margin-top: 15px; font-weight: 400; text-align: center; margin-bottom: 8px;">Muhayy Ud Din<sup>a</sup>, Waseem Akram<sup>a</sup>, Lyes Saad Saoud<sup>a</sup>, Jan Rosell<sup>b</sup>, Irfan Hussain<sup>*a</sup></p>
            <p style="font-size: 0.9rem; color: rgba(255, 255, 255, 0.85); margin-top: 0; line-height: 1.4; text-align: center; margin-bottom: 3px;"><sup>a</sup>Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University, United Arab Emirates</p>
            <p style="font-size: 0.9rem; color: rgba(255, 255, 255, 0.85); margin-top: 0; line-height: 1.4; text-align: center; margin-bottom: 8px;"><sup>b</sup>Institute of Industrial and Control Engineering (IOC), Universitat Politecnica de Catalunya, Spain</p>
            <p style="font-size: 1rem; color: rgba(255, 255, 255, 0.85); margin-top: 0; font-style: italic; text-align: center;">Accepted in Information Fusion Journal</p>
        </header>

        <div class="menu-bar">
            <a href="https://github.com/Muhayyuddin/VLAs" target="_blank">üìÅ GitHub</a>
            <a href="assets/manuscript.pdf" target="_blank">üìÑ Paper</a>
            <a href="#table-of-contents">üìë Table of Contents</a>
        </div>

        <section>
            <p style="font-size: 1.1rem; line-height: 1.6; color: #34495e; margin-bottom: 20px;">A comprehensive resource page for Vision-Language-Action models, datasets, and evaluation tools in robotic manipulation research. This page accompanies the paper "Multimodal Fusion with Vision-Language-Action Models for Robotic Manipulation: A Systematic Review" published in Information Fusion Journal, and provides a living catalog of research resources. We aim to keep this collection up to date as new VLA models, datasets, and simulation tools emerge. Contributions and pull requests to our GitHub repository adding recently published work or tooling are most welcome!</p>
            
            <h3>Abstract</h3>
            <p style="text-align: justify; background-color: #f8f9fa; padding: 20px; border-radius: 6px; border-left: 4px solid #667eea; font-style: italic; line-height: 1.6;">
Vision Language Action (VLA) models represent a new frontier in robotics by unifying perception, reasoning,
and control within a single multimodal learning framework. By jointly leveraging visual, linguistic, and motor
modalities, they enable instruction-driven manipulation, cross-embodiment generalization, and scalable auton-
omy. This systematic review synthesizes the state of the art in VLA research with an emphasis on architectures,
algorithms, and applications relevant to robotic manipulation. We examine 102 models, 26 foundational
datasets, and 12 simulation platforms, categorizing them according to their fusion strategies and integration
mechanisms. Foundational datasets are evaluated using a novel criterion based on task complexity, modality
richness, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning.
We further introduce a structured taxonomy of fusion hierarchies and encoderdecoder families, together with a
two-dimensional dataset characterization framework and a meta-analytic benchmarking protocol that quanti-
tatively link design variables to empirical performance across benchmarks. Our analysis shows that hierarchical
and late fusion architectures yield the highest manipulation success and generalization, confirming the benefit
of multi-level cross-modal integration. Diffusion-based decoders demonstrate superior cross-domain transfer
and robustness compared to autoregressive heads. Dataset analysis highlights a persistent lack of benchmarks
that combine high-complexity, multimodal, and long-horizon tasks, while existing simulators offer limited
multimodal synchronization and real-to-sim consistency. To address these gaps, we propose the VLA Fusion
Evaluation Benchmark to quantify fusion efficiency and alignment. Drawing on both academic and industrial
advances, the review outlines future research directions in adaptive and modular fusion architectures, compu-
tational resource optimization, and the deployment of interpretable, resource-efficient robotic systems. This
work provides both a conceptual foundation and a quantitative roadmap for advancing embodied intelligence
through multimodal information fusion across robotic domains.            </p>
            
            <h3>Citation</h3>
            <div class="citation-box">
                <pre><code>@article{din2025multimodal,
  title={Multimodal Fusion with Vision-Language-Action Models for Robotic Manipulation: A Systematic Review},
  author={Muhayy, Ud Din and Akram, Waseem and Saoud, Lyes Saad and Rosell, Jan and Hussain, Irfan},
  journal={Information Fusion},
  year={2025},
  publisher={Elsevier}
}</code></pre>
            </div>

            <img src="assets/applications.png" alt="VLA Applications Overview">
            <p>Vision-Language-Action models have found diverse applications across robotics domains, from manipulation and navigation to human-robot interaction and autonomous systems. The following figure illustrates the broad spectrum of VLA applications in real-world scenarios:</p>
        </section>

        <nav id="table-of-contents">
            <h2>Table of Contents</h2>
            <ul>
                <li><a href="#dataset-benchmarking">Dataset Benchmarking Code</a></li>
                <li><a href="#evaluation-visualization">VLA Models Evaluation & Visualization</a></li>
                <li><a href="#vla-models">VLA Models</a></li>
                <li><a href="#datasets">Datasets</a></li>
                <li><a href="#simulators">Simulators</a></li>
            </ul>
        </nav>

        <section id="dataset-benchmarking">
            <h2>Dataset Benchmarking Code</h2>
            <p>Benchmarking VLA Datasets by Task Complexity and Modality Richness. Each bubble represents a VLA dataset, positioned according to its normalized task-complexity score (x-axis) and its modality-richness score (y-axis). The bubble area is proportional to the dataset scale that is number of annotated episodes or interactions.</p>
            
            <img src="assets/benchmarkdataset.png" alt="Dataset Benchmarking Visualization">
            
            <p><a href="Plot_script/dataset_plot.py" target="_blank">View Code</a></p>
        </section>

        <section id="evaluation-visualization">
            <h2>VLA Models Evaluation & Visualization</h2>
            <p>This repository includes a comprehensive analysis and visualization suite for evaluating Vision-Language-Action (VLA) models. The analysis covers multiple aspects of VLA model performance, architecture components, and theoretical foundations through detailed visualizations and statistical analysis.</p>
            
            <p><a href="Plot_script/final_plots.py" target="_blank">View Code</a></p>

            <h3>Representative Visualizations</h3>
            <div class="plot-grid">
                <div class="plot-item">
                    <img src="Plot_script/plots/forest_plot.png" alt="Forest Plot Analysis">
                    <h4>Forest Plot Analysis</h4>
                    <p>Regression analysis showing that diffusion-based decoders and hierarchical fusion strategies provide the strongest positive impact on manipulation success, while symbolic/MLP controllers show degraded performance under real-world conditions.</p>
                </div>
                <div class="plot-item">
                    <img src="Plot_script/plots/encoder_analysis_4panel.png" alt="Encoder Analysis">
                    <h4>Encoder Analysis</h4>
                    <p>Performance comparison across vision and language encoders. SigLIP and DINO vision encoders achieve highest success, while mid-scale instruction-tuned language models (T5, LLaMA, Qwen) provide optimal balance between task success and generalization.</p>
                </div>
                <div class="plot-item">
                    <img src="Plot_script/plots/domain_component_analysis_4panel.png" alt="Domain Analysis">
                    <h4>Domain Component Analysis</h4>
                    <p>Cross-domain performance analysis across humanoid, manipulation, and navigation tasks. Diffusion decoders consistently achieve higher success and generalization, demonstrating superior robustness for temporally coherent, cross-modal control.</p>
                </div>
                <div class="plot-item">
                    <img src="Plot_script/plots/VLA_FEB_Hist.png" alt="VLA-FEB Score Distribution">
                    <h4>VLA-FEB Score Distribution</h4>
                    <p>Composite scores from the VLA-FEB framework evaluating fusion efficiency, generalization, real-to-sim transfer, and cross-modal alignment. Hierarchical and diffusion-based models achieve highest performance across all evaluation dimensions.</p>
                </div>
                <div class="plot-item">
                    <img src="Plot_script/plots/scale_analysis_4panel.png" alt="Scale Analysis">
                    <h4>Scale Analysis</h4>
                    <p>Analysis of model scale versus fusion depth impact on success rates. Results show that deeper fusion and hierarchical architectures consistently outperform pure parameter scaling, indicating that architectural design matters more than model size.</p>
                </div>
                <div class="plot-item">
                    <img src="Plot_script/plots/vla_fusion_theory_visualization_3panel.png" alt="VLA Fusion Theory">
                    <h4>VLA Fusion Theory</h4>
                    <p>Quantitative visualization of fusion dynamics: (a) Entropy reduction showing progressive uncertainty reduction, (b) Cross-modal attention efficiency across fusion types, (c) Fusion energy correlation with task success, demonstrating hierarchical fusion superiority.</p>
                </div>
            </div>
        </section>

        <section id="quick-start">
            <h2>Quick Start</h2>
            
            <h3>1. Create Virtual Environment (Recommended)</h3>
            <pre><code>python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate</code></pre>
            
            <h3>2. Install Dependencies</h3>
            <pre><code>pip install -r requirements.txt</code></pre>
            
            <h3>3. Run Analysis</h3>
            <pre><code>python final_plots.py</code></pre>
        </section>

        <section id="vla-models">
            <h2>VLA Models</h2>
            <img src="assets/VLA.png" alt="VLA Models Trend">
            <p>The top row presents major VLA models introduced each year, alongside their associated institutions. The bottom row displays key datasets used to train and evaluate VLA models, grouped by release year. The figure highlights the increasing scale and diversity of datasets and institutional involvement, with contributions from academic (e.g., CMU, CNRS, UC, Peking Uni) and industrial labs (e.g., Google, NVIDIA, Microsoft). This timeline highlights the rapid advancements in VLA research.</p>
            
            <p>Below is the list of the VLAs reviewed in the paper:</p>

            <div class="models-grid">
                <a href="https://proceedings.mlr.press/v164/shridhar22a/shridhar22a.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2022</span>Cliport: What and where pathways for robotic manipulation
                </a>
                <a href="https://arxiv.org/abs/2212.06817" class="model-link" target="_blank">
                    <span class="year-badge">2022</span>Rt-1: Robotics transformer for real‚Äëworld control at scale
                </a>
                <a href="https://arxiv.org/abs/2205.06175" class="model-link" target="_blank">
                    <span class="year-badge">2022</span>A Generalist Agent
                </a>
                <a href="https://arxiv.org/abs/2210.03094" class="model-link" target="_blank">
                    <span class="year-badge">2022</span>VIMA: General Robot Manipulation with Multimodal Prompts
                </a>
                <a href="https://peract.github.io/paper/peract_corl2022.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2022</span>PERCEIVER-ACTOR: A Multi-Task Transformer for Robotic Manipulation
                </a>
                <a href="https://arxiv.org/abs/2204.01691" class="model-link" target="_blank">
                    <span class="year-badge">2022</span>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances
                </a>
                <a href="https://arxiv.org/abs/2310.08560" class="model-link" target="_blank">
                    <span class="year-badge">2023</span>RoboAgent: Generalist Robot Agent with Semantic and Temporal Understanding
                </a>
                <a href="https://arxiv.org/abs/2311.01977" class="model-link" target="_blank">
                    <span class="year-badge">2023</span>Robotic Task Generalization via Hindsight Trajectory Sketches
                </a>
                <a href="https://arxiv.org/abs/2304.13705" class="model-link" target="_blank">
                    <span class="year-badge">2023</span>Learning fine‚Äëgrained bimanual manipulation with low‚Äëcost hardware
                </a>
                <a href="#" class="model-link" target="_blank">
                    <span class="year-badge">2023</span>Rt-2: Vision‚Äëlanguage‚Äëaction models transfer web knowledge to robotic control
                </a>
                <a href="https://arxiv.org/abs/2307.05973" class="model-link" target="_blank">
                    <span class="year-badge">2023</span>Voxposer: Composable 3D value maps for robotic manipulation with language models
                </a>
                <a href="https://arxiv.org/abs/2411.00508" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>CLIP‚ÄëRT: Learning Language‚ÄëConditioned Robotic Policies with Natural Language Supervision
                </a>
                <a href="https://arxiv.org/pdf/2303.04137" class="model-link" target="_blank">
                    <span class="year-badge">2023</span>Diffusion Policy: Visuomotor policy learning via action diffusion
                </a>
                <a href="https://arxiv.org/abs/2405.12213" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>Octo: An open‚Äësource generalist robot policy
                </a>
                <a href="https://arxiv.org/abs/2409.12894" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>Towards testing and evaluating vision‚Äëlanguage manipulation: An empirical study
                </a>
                <a href="https://arxiv.org/abs/2412.04453" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>NaVILA: Legged robot vision‚Äëlanguage‚Äëaction model for navigation
                </a>
                <a href="https://arxiv.org/pdf/2409.19590" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>RoboNurse‚ÄëVLA: Real‚Äëtime voice‚Äëto‚Äëaction pipeline for surgical instrument handover
                </a>
                <a href="https://arxiv.org/pdf/2407.07775" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>Mobility VLA: Multimodal instruction navigation with topological mapping
                </a>
                <a href="https://arxiv.org/pdf/2409.15250.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>ReVLA: Domain adaptation adapters for robotic foundation models
                </a>
                <a href="https://arxiv.org/pdf/2412.06224.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>Uni‚ÄëNaVid: Video‚Äëbased VLA unifying embodied navigation tasks
                </a>
                <a href="https://arxiv.org/pdf/2410.07864.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>RDT‚Äë1B: 1.2B‚Äëparameter diffusion foundation model for manipulation
                </a>
                <a href="https://arxiv.org/pdf/2406.04339.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>RoboMamba: Mamba‚Äëbased unified VLA with linear‚Äëtime inference
                </a>
                <a href="https://arxiv.org/pdf/2412.20451.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>Chain‚Äëof‚ÄëAffordance: Sequential affordance reasoning for spatial planning
                </a>
                <a href="https://arxiv.org/pdf/2403.04908" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>Edge VLA: Self-Adapting Large Visual-Language Models to Edge Devices across Visual Modalities
                </a>
                <a href="https://arxiv.org/pdf/2406.09246.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>OpenVLA: LORA‚Äëfine‚Äëtuned open‚Äësource VLA with high‚Äësuccess transfer
                </a>
                <a href="https://arxiv.org/pdf/2411.19650.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>CogACT: Componentized diffusion action transformer for VLA
                </a>
                <a href="https://arxiv.org/pdf/2411.17465" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>ShowUI‚Äë2B: GUI/web navigation via screenshot grounding and token selection
                </a>
                <a href="https://arxiv.org/pdf/2410.05273" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>HiRT: Hierarchical planning/control separation for VLA
                </a>
                <a href="https://arxiv.org/pdf/2410.24164.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>Pi‚Äë0: General robot control flow model for open‚Äëworld tasks
                </a>
                <a href="https://arxiv.org/pdf/2406.07549.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>A3VLM: Articulation‚Äëaware affordance grounding from RGB video
                </a>
                <a href="https://arxiv.org/pdf/2502.01071.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>SVLR: Modular "segment‚Äëto‚Äëaction" pipeline using visual prompt retrieval
                </a>
                <a href="https://arxiv.org/pdf/2405.06039.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>Bi‚ÄëVLA: Dual‚Äëarm instruction‚Äëto‚Äëaction planner for recipe demonstrations
                </a>
                <a href="https://arxiv.org/pdf/2312.14457.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>QUAR‚ÄëVLA: Quadruped‚Äëspecific VLA with adaptive gait mapping
                </a>
                <a href="https://arxiv.org/pdf/2403.09631" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>3D‚ÄëVLA: Integrating 3D generative diffusion heads for world reconstruction
                </a>
                <a href="https://arxiv.org/pdf/2412.07215.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>RoboMM: MIM‚Äëbased multimodal decoder unifying 3D perception and language
                </a>
                <a href="https://arxiv.org/pdf/2409.16578.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>FLaRe: Large-Scale RL Fine-Tuning for Adaptive Robotic Policies
                </a>
                <a href="https://arxiv.org/pdf/2409.16578.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>GRAPE: Preference‚ÄëGuided Policy Adaptation via Feedback
                </a>
                <a href="https://arxiv.org/pdf/2410.15959.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>Diffusion Transformer Policy: Robust Multimodal Action Sampling
                </a>
                <a href="https://arxiv.org/pdf/2412.03293.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>Diffusion‚ÄëVLA: Diffusion‚ÄëBased Policy for Generalizable Manipulation
                </a>
                <a href="https://arxiv.org/pdf/2501.09747.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>FAST: Frequency‚Äëspace action tokenization for faster inference
                </a>
                <a href="https://arxiv.org/pdf/2502.19645.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>OpenVLA‚ÄëOFT: Optimized fine‚Äëtuning of OpenVLA with parallel decoding
                </a>
                <a href="https://arxiv.org/pdf/2408.10845.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>CoVLA: Autonomous driving VLA trained on annotated scene data
                </a>
                <a href="https://arxiv.org/pdf/2503.19755.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>ORION: Holistic end‚Äëto‚Äëend driving VLA with semantic trajectory control
                </a>
                <a href="https://arxiv.org/pdf/2501.05014.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>UAV‚ÄëVLA: Zero‚Äëshot aerial mission VLA combining satellite/UAV imagery
                </a>
                <a href="https://arxiv.org/pdf/2503.09527.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Combat VLA: Ultra‚Äëfast tactical reasoning in 3D environments
                </a>
                <a href="https://arxiv.org/pdf/2503.10631.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>HybridVLA: Ensemble decoding combining diffusion and autoregressive policies
                </a>
                <a href="https://arxiv.org/pdf/2504.19854.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>NORA: Low‚Äëoverhead VLA with integrated visual reasoning and FAST decoding
                </a>
                <a href="https://arxiv.org/pdf/2501.15830.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>SpatialVLA: 3D spatial encoding and adaptive action discretization
                </a>
                <a href="https://arxiv.org/pdf/2503.20384.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>MoLe‚ÄëVLA: Selective layer activation for faster inference
                </a>
                <a href="https://arxiv.org/pdf/2503.16365.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>JARVIS‚ÄëVLA: Open‚Äëworld instruction following in 3D games with keyboard/mouse
                </a>
                <a href="https://arxiv.org/pdf/2501.18867.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>UP‚ÄëVLA: Unified understanding and prediction model for embodied agents
                </a>
                <a href="https://arxiv.org/pdf/2501.06919.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Shake‚ÄëVLA: Modular bimanual VLA for cocktail‚Äëmixing tasks
                </a>
                <a href="https://arxiv.org/pdf/2503.08007.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>MORE: Scalable mixture‚Äëof‚Äëexperts RL for VLA models
                </a>
                <a href="https://arxiv.org/pdf/2502.20900.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>DexGraspVLA: Diffusion‚Äëbased dexterous grasping framework
                </a>
                <a href="https://arxiv.org/pdf/2502.05855.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>DexVLA: Cross‚Äëembodiment diffusion expert for rapid adaptation
                </a>
                <a href="https://arxiv.org/pdf/2502.14795.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Humanoid‚ÄëVLA: Hierarchical full‚Äëbody humanoid control VLA
                </a>
                <a href="https://arxiv.org/pdf/2502.19250.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>ObjectVLA: End‚Äëto‚Äëend open‚Äëworld object manipulation
                </a>
                <a href="https://arxiv.org/pdf/2503.20020.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Gemini Robotics: Bringing AI into the Physical World
                </a>
                <a href="https://arxiv.org/pdf/2407.08693.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>ECoT: Robotic Control via Embodied Chain‚Äëof‚ÄëThought Reasoning
                </a>
                <a href="https://arxiv.org/pdf/2503.03734.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>OTTER: A Vision‚ÄëLanguage‚ÄëAction Model with Text‚ÄëAware Visual Feature Extraction
                </a>
                <a href="https://arxiv.org/pdf/2504.16054.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>œÄ‚Äë0.5: A VLA Model with Open‚ÄëWorld Generalization
                </a>
                <a href="https://arxiv.org/pdf/2505.11917.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>OneTwoVLA: A Unified Model with Adaptive Reasoning
                </a>
                <a href="https://www.figure.ai/news/helix" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Helix: A Vision-Language-Action Model for Generalist Humanoid Control
                </a>
                <a href="https://arxiv.org/pdf/2506.01844.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>SmolVLA: A Vision‚ÄëLanguage‚ÄëAction Model for Affordable and Efficient Robotics
                </a>
                <a href="https://openreview.net/pdf/32c153a3b16174884cf62b285adbfbdcc57b163e.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>EF‚ÄëVLA: Vision‚ÄëLanguage‚ÄëAction Early Fusion with Causal Transformers
                </a>
                <a href="https://arxiv.org/pdf/2503.02310.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>PD‚ÄëVLA: Accelerating vision‚Äëlanguage‚Äëaction inference via parallel decoding
                </a>
                <a href="https://arxiv.org/pdf/2506.13751.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>LeVERB: Humanoid Whole‚ÄëBody Control via Latent Verb Generation
                </a>
                <a href="https://arxiv.org/pdf/2503.08548.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>TLA: Tactile‚ÄëLanguage‚ÄëAction Model for High‚ÄëPrecision Contact Tasks
                </a>
                <a href="https://arxiv.org/pdf/2505.02152.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Interleave‚ÄëVLA: Enhancing VLM‚ÄëLLM interleaved instruction processing
                </a>
                <a href="https://arxiv.org/pdf/2501.16664.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>iRe‚ÄëVLA: Iterative reinforcement and supervised fine‚Äëtuning for robust VLA
                </a>
                <a href="https://arxiv.org/pdf/2412.10345.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>TraceVLA: Visual trace prompting for spatio‚Äëtemporal manipulation cues
                </a>
                <a href="https://arxiv.org/pdf/2503.23463.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>OpenDrive VLA: End‚Äëto‚ÄëEnd Driving with Semantic Scene Alignment
                </a>
                <a href="https://arxiv.org/pdf/2506.09985.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>V‚ÄëJEPA 2: Dual‚ÄëStream Video JEPA for Predictive Robotic Planning
                </a>
                <a href="https://arxiv.org/pdf/2505.23705.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Knowledge Insulating VLA: Insulation Layers for Modular VLA Training
                </a>
                <a href="https://arxiv.org/pdf/2503.14734.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>GR00T N1: Diffusion Foundation Model for Humanoid Control
                </a>
                <a href="https://arxiv.org/pdf/2503.06669.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>AgiBot World Colosseo: Unified Embodied Dataset Platform
                </a>
                <a href="https://arxiv.org/pdf/2502.19417.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Hi Robot: Hierarchical Planning and Control for Complex Environments
                </a>
                <a href="https://arxiv.org/pdf/2501.01895.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>EnerVerse: World‚ÄëModel LLM for Long‚ÄëHorizon Manipulation
                </a>
                <a href="https://arxiv.org/pdf/2501.04693.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Beyond Sight: Sensor Fusion via Language-Grounded Attention
                </a>
                <a href="https://arxiv.org/pdf/2501.09783.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>GeoManip: Geometric Constraint Encoding for Robust Manipulation
                </a>
                <a href="https://arxiv.org/pdf/2501.10105.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Universal Actions: Standardizing Action Dictionaries for Transfer
                </a>
                <a href="https://arxiv.org/pdf/2501.06605.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>RoboHorizon: Multi-View Environment Modeling with LLM Planning
                </a>
                <a href="https://arxiv.org/pdf/2501.18564.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>SAM2Act: Segmentation‚ÄëAugmented Memory for Object‚ÄëCentric Manipulation
                </a>
                <a href="https://arxiv.org/pdf/2502.02175.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>VLA‚ÄëCache: Token Caching for Efficient VLA Inference
                </a>
                <a href="https://arxiv.org/pdf/2502.01828.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Forethought VLA: Latent Alignment for Foresight‚ÄëDriven Policies
                </a>
                <a href="https://arxiv.org/pdf/2502.05485.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>HAMSTER: Hierarchical Skill Decomposition for Multi‚ÄëStep Manipulation
                </a>
                <a href="https://arxiv.org/pdf/2507.10672v1" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>TempoRep VLA: Successor Representation for Temporal Planning
                </a>
                <a href="https://arxiv.org/pdf/2502.05450.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>ConRFT: Consistency Regularized Fine‚ÄëTuning with Reinforcement
                </a>
                <a href="https://arxiv.org/pdf/2502.07837.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>RoboBERT: Unified Multimodal Transformer for Manipulation
                </a>
                <a href="https://arxiv.org/pdf/2502.09268.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>GEVRM: Generative Video Modeling for Goal‚ÄëOriented Planning
                </a>
                <a href="https://arxiv.org/pdf/2502.13143.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>SoFar: Successor‚ÄëFeature Orientation Representations
                </a>
                <a href="https://arxiv.org/pdf/2502.13142.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>ARM4R: Auto‚ÄëRegressive 4D Transition Modeling for Trajectories
                </a>
                <a href="https://arxiv.org/pdf/2502.13130.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Magma: Foundation Multimodal Agent Model for Control
                </a>
                <a href="https://arxiv.org/pdf/2501.15068.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>An Atomic Skill Library: Modular Skill Composition for Robotics
                </a>
                <a href="https://arxiv.org/pdf/2502.21257.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>RoboBrain: Knowledge‚ÄëGrounded Policy Brain for Multimodal Tasks
                </a>
                <a href="https://arxiv.org/pdf/2503.03480.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>SafeVLA: Safety‚ÄëAware Vision‚ÄëLanguage‚ÄëAction Policies
                </a>
                <a href="https://arxiv.org/pdf/2503.01378.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>CognitiveDrone: Embodied Reasoning VLA for UAV Planning
                </a>
                <a href="https://arxiv.org/pdf/2502.13508.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>VLAS: Voice‚ÄëDriven Vision‚ÄëLanguage‚ÄëAction Control
                </a>
                <a href="https://arxiv.org/pdf/2502.14420.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>ChatVLA: Conversational VLA for Interactive Control
                </a>
                <a href="https://arxiv.org/pdf/2506.04308.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics
                </a>
            </div>
        </section>

        <section id="datasets">
            <h2>Datasets</h2>
            <p>Comprehensive collection of datasets used for training and evaluating VLA models:</p>
            <div class="models-grid">
                <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Das_Embodied_Question_Answering_CVPR_2018_paper.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2018</span>EmbodiedQA: Embodied Question Answering
                </a>
                <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2018</span>R2R: Vision‚Äëand‚ÄëLanguage Navigation: Interpreting Visually‚ÄëGrounded Navigation Instructions in Real Environments
                </a>
                <a href="https://arxiv.org/abs/1907.04957" class="model-link" target="_blank">
                    <span class="year-badge">2019</span>Vision‚Äëand‚ÄëDialog Navigation
                </a>
                <a href="https://arxiv.org/abs/1912.01734" class="model-link" target="_blank">
                    <span class="year-badge">2020</span>ALFRED
                </a>
                <a href="https://arxiv.org/pdf/1909.12271.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2020</span>RLBench: The Robot Learning Benchmark & Learning Environment
                </a>
                <a href="https://arxiv.org/abs/2110.00534" class="model-link" target="_blank">
                    <span class="year-badge">2021</span>TEACh: Task‚Äëdriven Embodied Agents that Chat
                </a>
                <a href="https://arxiv.org/pdf/2202.13330.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2022</span>DialFRED: Dialogue‚ÄëEnabled Agents for Embodied Instruction Following
                </a>
                <a href="https://arxiv.org/abs/2110.07058" class="model-link" target="_blank">
                    <span class="year-badge">2022</span>Ego4D: Around the World in 3,000 Hours of Egocentric Video
                </a>
                <a href="https://arxiv.org/abs/2112.03227" class="model-link" target="_blank">
                    <span class="year-badge">2022</span>CALVIN: A Benchmark for Language‚ÄëConditioned Long‚ÄëHorizon Robot Manipulation Tasks
                </a>
                <a href="https://proceedings.mlr.press/v229/walke23a/walke23a.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2023</span>BridgeData V2: A Dataset for Robot Learning at Scale
                </a>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/8c3c666820ea055a77726d66fc7d447f-Paper-Datasets_and_Benchmarks.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2023</span>LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning
                </a>
                <a href="https://arxiv.org/abs/2312.06686" class="model-link" target="_blank">
                    <span class="year-badge">2023</span>Robo360: A 3D Omnispective Multi‚ÄëModal Robotic Manipulation Dataset
                </a>
                <a href="https://droid-dataset.github.io/" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>DROID: A Large‚ÄëScale In‚ÄëThe‚ÄëWild Robot Manipulation Dataset
                </a>
                <a href="https://arxiv.org/abs/2408.10845" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>CoVLA: Comprehensive Vision‚ÄëLanguage‚ÄëAction Dataset for Autonomous Driving
                </a>
                <a href="https://arxiv.org/abs/2412.07215" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>RoboMM: All‚Äëin‚ÄëOne Multimodal Large Model for Robotic Manipulation
                </a>
                <a href="https://arxiv.org/abs/2408.10899" class="model-link" target="_blank">
                    <span class="year-badge">2024</span>All Robots in One: A New Standard and Unified Dataset for Versatile, General‚ÄëPurpose Embodied Agents
                </a>
                <a href="https://arxiv.org/abs/2310.08864" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Open X-Embodiment: Robotic Learning Datasets and RT‚ÄëX Models
                </a>
                <a href="https://arxiv.org/abs/2411.16537" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>RoboSpatial: Teaching Spatial Understanding via Vision‚ÄëLanguage Models for Robotics
                </a>
                <a href="https://arxiv.org/abs/2503.08548" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>TLA: Tactile‚ÄëLanguage‚ÄëAction Model for Contact‚ÄëRich Manipulation
                </a>
                <a href="https://arxiv.org/abs/2503.05231" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Kaiwu: A Multimodal Manipulation Dataset and Framework for Robotic Perception and Interaction
                </a>
                <a href="https://arxiv.org/abs/2505.12707" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>PLAICraft: Large‚ÄëScale Time‚ÄëAligned Vision‚ÄëSpeech‚ÄëAction Dataset for Embodied AI
                </a>
                <a href="https://arxiv.org/abs/2503.06669" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>AgiBot World Colosseo: A Large‚ÄëScale Manipulation Dataset for Intelligent Embodied Systems
                </a>
                <a href="https://arxiv.org/abs/2502.05086" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>REASSEMBLE: A Multimodal Dataset for Contact‚ÄëRich Robotic Assembly and Disassembly
                </a>
                <a href="https://arxiv.org/abs/2506.06677" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>RoboCerebra: A Large‚ÄëScale Benchmark for Long‚ÄëHorizon Robotic Manipulation Evaluation
                </a>
                <a href="https://arxiv.org/abs/2503.17406" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>IRef‚ÄëVLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes
                </a>
                <a href="https://arxiv.org/abs/2406.07000" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>Interleave‚ÄëVLA: Enhancing Robot Manipulation with Interleaved Image‚ÄëText Instructions
                </a>
                <a href="https://arxiv.org/pdf/2506.04308.pdf" class="model-link" target="_blank">
                    <span class="year-badge">2025</span>RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics
                </a>
            </div>
        </section>

        <section id="simulators">
            <h2>Simulators</h2>
            <p>Widely adopted simulation platforms for generating VLA data:</p>
            <div class="models-grid">
                <a href="http://gazebosim.org" class="model-link" target="_blank">
                    <span class="year-badge">2004</span>Gazebo
                </a>
                <a href="https://cyberbotics.com" class="model-link" target="_blank">
                    <span class="year-badge">2004</span>Webots
                </a>
                <a href="https://mujoco.org" class="model-link" target="_blank">
                    <span class="year-badge">2012</span>MuJoCo
                </a>
                <a href="https://www.coppeliarobotics.com" class="model-link" target="_blank">
                    <span class="year-badge">2013</span>CoppeliaSim
                </a>
                <a href="https://pybullet.org" class="model-link" target="_blank">
                    <span class="year-badge">2016</span>PyBullet
                </a>
                <a href="https://ai2thor.allenai.org" class="model-link" target="_blank">
                    <span class="year-badge">2017</span>AI2-THOR
                </a>
                <a href="https://unity-technologies.github.io/ml-agents/" class="model-link" target="_blank">
                    <span class="year-badge">2018</span>Unity ML‚ÄëAgents
                </a>
                <a href="https://aihabitat.org" class="model-link" target="_blank">
                    <span class="year-badge">2019</span>Habitat
                </a>
                <a href="https://developer.nvidia.com/isaac-sim" class="model-link" target="_blank">
                    <span class="year-badge">2020</span>NVIDIA Isaac Sim
                </a>
                <a href="https://svl.stanford.edu/igibson" class="model-link" target="_blank">
                    <span class="year-badge">2020</span>iGibson
                </a>
                <a href="https://sapien.ucsd.edu" class="model-link" target="_blank">
                    <span class="year-badge">2020</span>SAPIEN
                </a>
                <a href="https://universal-simulator.github.io/unisim/" class="model-link" target="_blank">
                    <span class="year-badge">2023</span>UniSim
                </a>
            </div>
        </section>

        <footer>
            <p>¬© 2025 VLA Models Repository. This is a living document that will be updated as new research emerges.</p>
            <p>Contributions and pull requests are welcome!</p>
        </footer>
    </div>
</body>
</html>
