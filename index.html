<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VLA-Bench: Vision-Language-Action Model Benchmark</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: rgba(255, 255, 255, 0.95);
            padding: 2rem;
            border-radius: 15px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
            margin-bottom: 2rem;
            text-align: center;
        }

        h1 {
            color: #667eea;
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.1);
        }

        .subtitle {
            color: #666;
            font-size: 1.2rem;
            margin-bottom: 1rem;
        }

        .authors {
            color: #555;
            font-size: 1rem;
            margin-bottom: 1rem;
        }

        .links {
            margin-top: 1.5rem;
        }

        .links a {
            display: inline-block;
            margin: 0.5rem;
            padding: 0.75rem 1.5rem;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-decoration: none;
            border-radius: 25px;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
        }

        .links a:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(0, 0, 0, 0.3);
        }

        .section {
            background: rgba(255, 255, 255, 0.95);
            padding: 2rem;
            border-radius: 15px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
            margin-bottom: 2rem;
        }

        h2 {
            color: #667eea;
            margin-bottom: 1rem;
            font-size: 2rem;
            border-bottom: 3px solid #667eea;
            padding-bottom: 0.5rem;
        }

        h3 {
            color: #764ba2;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
            font-size: 1.5rem;
        }

        .model-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin-top: 1.5rem;
        }

        .model-card {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 1.5rem;
            border-radius: 10px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
        }

        .model-card:hover {
            transform: translateY(-5px);
        }

        .model-card h4 {
            color: #667eea;
            margin-bottom: 0.5rem;
        }

        .visualization {
            margin: 2rem 0;
            text-align: center;
        }

        .visualization img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2);
            margin: 1rem 0;
        }

        .benchmark-table {
            overflow-x: auto;
            margin: 1.5rem 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 10px;
            overflow: hidden;
        }

        th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 1rem;
            text-align: left;
        }

        td {
            padding: 1rem;
            border-bottom: 1px solid #eee;
        }

        tr:hover {
            background: #f5f7fa;
        }

        .highlight {
            background: linear-gradient(135deg, #ffeaa7 0%, #fdcb6e 100%);
            padding: 1.5rem;
            border-radius: 10px;
            margin: 1.5rem 0;
            border-left: 5px solid #fdcb6e;
        }

        .code-block {
            background: #2d3436;
            color: #dfe6e9;
            padding: 1.5rem;
            border-radius: 10px;
            overflow-x: auto;
            margin: 1rem 0;
            font-family: 'Courier New', monospace;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.8rem;
            }

            .model-grid {
                grid-template-columns: 1fr;
            }
        }

        .abstract {
            text-align: justify;
            line-height: 1.8;
            color: #555;
        }

        footer {
            text-align: center;
            padding: 2rem;
            color: white;
            margin-top: 2rem;
        }

        .task-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 1rem;
            margin: 1.5rem 0;
        }

        .task-item {
            background: linear-gradient(135deg, #a29bfe 0%, #6c5ce7 100%);
            color: white;
            padding: 1rem;
            border-radius: 8px;
            text-align: center;
            font-weight: bold;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
        }

        /* New styles for representative visualizations */
        .viz-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin: 2rem 0;
        }

        .viz-card {
            background: white;
            border-radius: 12px;
            overflow: hidden;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            transition: all 0.3s ease;
        }

        .viz-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 30px rgba(0, 0, 0, 0.15);
        }

        .viz-card img {
            width: 100%;
            height: auto;
            display: block;
        }

        .viz-card-content {
            padding: 1.5rem;
        }

        .viz-card h4 {
            color: #667eea;
            margin-bottom: 0.5rem;
            font-size: 1.2rem;
        }

        .viz-card p {
            color: #666;
            font-size: 0.95rem;
            line-height: 1.6;
        }

        /* Enhanced image viewer */
        .image-viewer {
            position: relative;
            cursor: pointer;
        }

        .image-viewer:hover::after {
            content: 'üîç Click to enlarge';
            position: absolute;
            bottom: 10px;
            right: 10px;
            background: rgba(0, 0, 0, 0.7);
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            font-size: 0.85rem;
        }

        /* Modal for full-size images */
        .modal {
            display: none;
            position: fixed;
            z-index: 1000;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.9);
            justify-content: center;
            align-items: center;
        }

        .modal-content {
            max-width: 90%;
            max-height: 90%;
            object-fit: contain;
        }

        .close-modal {
            position: absolute;
            top: 20px;
            right: 40px;
            color: white;
            font-size: 40px;
            font-weight: bold;
            cursor: pointer;
        }

        .close-modal:hover {
            color: #667eea;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ü§ñ VLA-Bench</h1>
            <p class="subtitle">Comprehensive Benchmark for Vision-Language-Action Models</p>
            <p class="authors">Research Team | 2024</p>
            <div class="links">
                <a href="https://github.com/Muhayyuddin/VLAs" target="_blank">üìÅ GitHub Repository</a>
                <a href="#" target="_blank">üìÑ Paper (Coming Soon)</a>
                <a href="#" target="_blank">üé• Demo</a>
            </div>
        </header>

        <div class="section">
            <h2>üìã Abstract</h2>
            <p class="abstract">
                Vision-Language-Action (VLA) models represent a paradigm shift in robotics, combining visual perception, 
                natural language understanding, and action generation into unified frameworks. VLA-Bench provides a 
                comprehensive benchmark for evaluating these models across multiple dimensions including manipulation 
                performance, language grounding, visual understanding, and generalization capabilities. Our benchmark 
                includes diverse tasks ranging from simple pick-and-place operations to complex, multi-step manipulation 
                scenarios requiring sophisticated reasoning and planning.
            </p>
        </div>

        <div class="section">
            <h2>üéØ Key Features</h2>
            <div class="highlight">
                <h3>Why VLA-Bench?</h3>
                <ul style="margin-left: 2rem; margin-top: 1rem;">
                    <li><strong>Comprehensive Evaluation:</strong> 15+ diverse robotic manipulation tasks</li>
                    <li><strong>Multi-Modal Assessment:</strong> Vision, language, and action components</li>
                    <li><strong>Standardized Metrics:</strong> Consistent evaluation across different models</li>
                    <li><strong>Open Source:</strong> Fully reproducible with detailed documentation</li>
                    <li><strong>Extensible Framework:</strong> Easy to add new tasks and models</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>ü§ñ Evaluated Models</h2>
            <p>We evaluate state-of-the-art VLA models across our comprehensive benchmark suite:</p>
            
            <div class="model-grid">
                <div class="model-card">
                    <h4>OpenVLA</h4>
                    <p><strong>Architecture:</strong> Transformer-based</p>
                    <p><strong>Parameters:</strong> 7B</p>
                    <p><strong>Training:</strong> Open X-Embodiment</p>
                </div>
                
                <div class="model-card">
                    <h4>Octo</h4>
                    <p><strong>Architecture:</strong> Generalist Policy</p>
                    <p><strong>Parameters:</strong> 93M</p>
                    <p><strong>Training:</strong> Multi-robot datasets</p>
                </div>
                
                <div class="model-card">
                    <h4>RT-2</h4>
                    <p><strong>Architecture:</strong> Vision-Language-Action</p>
                    <p><strong>Parameters:</strong> 55B</p>
                    <p><strong>Training:</strong> Web + Robotics data</p>
                </div>
                
                <div class="model-card">
                    <h4>RT-1</h4>
                    <p><strong>Architecture:</strong> Robotics Transformer</p>
                    <p><strong>Parameters:</strong> 35M</p>
                    <p><strong>Training:</strong> Real robot data</p>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>üìä Benchmark Tasks</h2>
            <p>Our benchmark includes diverse manipulation tasks across multiple categories:</p>
            
            <div class="task-grid">
                <div class="task-item">Pick & Place</div>
                <div class="task-item">Object Sorting</div>
                <div class="task-item">Door Opening</div>
                <div class="task-item">Drawer Manipulation</div>
                <div class="task-item">Tool Use</div>
                <div class="task-item">Assembly Tasks</div>
                <div class="task-item">Deformable Objects</div>
                <div class="task-item">Multi-Step Tasks</div>
            </div>

            <h3>Task Complexity Levels</h3>
            <ul style="margin-left: 2rem; margin-top: 1rem;">
                <li><strong>Level 1:</strong> Simple pick-and-place with clear visual targets</li>
                <li><strong>Level 2:</strong> Language-conditioned manipulation with object attributes</li>
                <li><strong>Level 3:</strong> Multi-step tasks requiring planning</li>
                <li><strong>Level 4:</strong> Complex reasoning with environmental constraints</li>
                <li><strong>Level 5:</strong> Novel object generalization and creative problem-solving</li>
            </ul>
        </div>

        <div class="section">
            <h2>üé® Representative Visualizations</h2>
            <p>Below are key visualizations from our analysis of Vision-Language-Action models:</p>
            
            <div class="viz-grid">
                <div class="viz-card">
                    <div class="image-viewer" onclick="openModal('viz1')">
                        <img id="viz1" src="images/encoder_analysis_4panel.png" alt="Encoder Analysis">
                    </div>
                    <div class="viz-card-content">
                        <h4>Encoder Analysis</h4>
                        <p>Comprehensive analysis of visual encoder architectures showing attention patterns, feature representations, layer-wise activations, and semantic clustering across different VLA models.</p>
                    </div>
                </div>

                <div class="viz-card">
                    <div class="image-viewer" onclick="openModal('viz2')">
                        <img id="viz2" src="images/domain_component_analysis_4panel.png" alt="Domain Component Analysis">
                    </div>
                    <div class="viz-card-content">
                        <h4>Domain Component Analysis</h4>
                        <p>Multi-faceted analysis of domain-specific components including task distribution, cross-domain transfer capabilities, specialization patterns, and generalization metrics.</p>
                    </div>
                </div>

                <div class="viz-card">
                    <div class="image-viewer" onclick="openModal('viz3')">
                        <img id="viz3" src="images/scale_analysis_4panel.png" alt="Scale Analysis">
                    </div>
                    <div class="viz-card-content">
                        <h4>Scale Analysis</h4>
                        <p>Investigation of model scaling effects showing performance vs parameters, computational efficiency, training dynamics, and the relationship between model size and task complexity.</p>
                    </div>
                </div>

                <div class="viz-card">
                    <div class="image-viewer" onclick="openModal('viz4')">
                        <img id="viz4" src="images/vla_fusion_theory_3panel.png" alt="VLA Fusion Theory">
                    </div>
                    <div class="viz-card-content">
                        <h4>VLA Fusion Theory</h4>
                        <p>Theoretical framework for vision-language-action fusion, illustrating cross-modal attention mechanisms, multimodal integration strategies, and the flow of information across modalities.</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>üìà Results</h2>
            <div class="benchmark-table">
                <table>
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Success Rate (%)</th>
                            <th>Language Grounding</th>
                            <th>Generalization</th>
                            <th>Inference Speed (Hz)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>OpenVLA</strong></td>
                            <td>78.5</td>
                            <td>0.82</td>
                            <td>0.71</td>
                            <td>15</td>
                        </tr>
                        <tr>
                            <td><strong>Octo</strong></td>
                            <td>72.3</td>
                            <td>0.76</td>
                            <td>0.68</td>
                            <td>30</td>
                        </tr>
                        <tr>
                            <td><strong>RT-2</strong></td>
                            <td>81.2</td>
                            <td>0.85</td>
                            <td>0.74</td>
                            <td>8</td>
                        </tr>
                        <tr>
                            <td><strong>RT-1</strong></td>
                            <td>68.7</td>
                            <td>0.72</td>
                            <td>0.62</td>
                            <td>25</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="highlight">
                <h3>Key Findings</h3>
                <ul style="margin-left: 2rem; margin-top: 1rem;">
                    <li>Larger models show better language grounding but slower inference</li>
                    <li>Generalization remains a key challenge across all models</li>
                    <li>Trade-offs exist between model size, performance, and deployment feasibility</li>
                    <li>Multi-task training significantly improves robustness</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>üöÄ Getting Started</h2>
            
            <h3>Installation</h3>
            <div class="code-block">
git clone https://github.com/Muhayyuddin/VLAs.git
cd VLAs
pip install -r requirements.txt
            </div>

            <h3>Running Evaluations</h3>
            <div class="code-block">
# Evaluate a model
python evaluate.py --model openvla --task all

# Run specific task
python evaluate.py --model octo --task pick_place

# Custom configuration
python evaluate.py --config configs/custom_eval.yaml
            </div>

            <h3>Adding New Models</h3>
            <div class="code-block">
# Implement the VLAModel interface
class MyVLAModel(VLAModel):
    def __init__(self, config):
        # Initialize your model
        pass
    
    def predict_action(self, observation, instruction):
        # Generate action predictions
        pass
            </div>
        </div>

        <div class="section">
            <h2>üìö Citation</h2>
            <div class="code-block">
@article{vlabench2024,
    title={VLA-Bench: A Comprehensive Benchmark for Vision-Language-Action Models},
    author={Research Team},
    journal={arXiv preprint},
    year={2024}
}
            </div>
        </div>

        <div class="section">
            <h2>ü§ù Contributing</h2>
            <p>We welcome contributions! Please see our <a href="https://github.com/Muhayyuddin/VLAs/blob/main/CONTRIBUTING.md">contribution guidelines</a>.</p>
            <ul style="margin-left: 2rem; margin-top: 1rem;">
                <li>Report bugs and issues</li>
                <li>Submit new tasks and benchmarks</li>
                <li>Add model implementations</li>
                <li>Improve documentation</li>
            </ul>
        </div>

        <footer>
            <p>¬© 2024 VLA-Bench | <a href="https://github.com/Muhayyuddin/VLAs" style="color: white;">GitHub</a></p>
        </footer>
    </div>

    <!-- Modal for image viewing -->
    <div id="imageModal" class="modal" onclick="closeModal()">
        <span class="close-modal">&times;</span>
        <img class="modal-content" id="modalImage">
    </div>

    <script>
        function openModal(imgId) {
            const modal = document.getElementById('imageModal');
            const modalImg = document.getElementById('modalImage');
            const img = document.getElementById(imgId);
            
            modal.style.display = 'flex';
            modalImg.src = img.src;
        }

        function closeModal() {
            document.getElementById('imageModal').style.display = 'none';
        }

        // Close modal with Escape key
        document.addEventListener('keydown', function(event) {
            if (event.key === 'Escape') {
                closeModal();
            }
        });
    </script>
</body>
</html>